{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Data Visualization\n",
    "Machine learning, and therefore a large part of AI, is based on statistical analysis of data. In this notebook, you'll examine some fundamental concepts related to data and data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data\n",
    "Statistics are based on data, which consist of a collection of pieces of information about things you want to study. This information can take the form of descriptions, quantities, measurements, and other observations. Typically, we work with related data items in a *dataset*, which often consists of a collection of *observations* or *cases*. Most commonly, we thing about this dataset as a table that consists of a row for each observation, and a column for each individual data point related to that observation - we variously call these data points *attributes* or *features*, and they each describe a specific characteristic of the thing we're observing.\n",
    "\n",
    "Let's take a look at a real example. In 1886, Francis Galton conducted a study into the relationship between heights of parents and their (adult) children. Run the Python code below to view the data he collected (you can safely ignore a deprecation warning if it is displayed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "df = sm.datasets.get_rdataset('GaltonFamilies', package='HistData').data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Data\n",
    "Now, let's take a closer look at this data (you can click the left margin next to the dataset to toggle between full height and a scrollable pane). There are 933 observations, each one recording information pertaining to an individual child. The information recorded consists of the following features:\n",
    "- **family**: An identifier for the family to which the child belongs.\n",
    "- **father**: The height of the father.\n",
    "- ** mother**: The height of the mother.\n",
    "- **midparentHeight**: The mid-point between the father and mother's heights (calculated as *(father + 1.08 x mother) &div; 2*)\n",
    "- **children**: The total number of children in the family.\n",
    "- **childNum**: The number of the child to whom this observation pertains (Galton numbered the children in desending order of height, with male children listed before female children)\n",
    "- **gender**: The gender of the child to whom this observation pertains.\n",
    "- **childHeight**: The height of the child to whom this observation pertains.\n",
    "\n",
    "It's worth noting that there are several distinct types of data recorded here. To begin with, there are some features that represent *qualities*, or characteristics of the child - for example, gender. Other feaures represent a *quantity* or measurement, such as the child's height. So broadly speaking, we can divide data into *qualitative* and *quantitative* data.\n",
    "\n",
    "#### Qualitative Data\n",
    "Let's take a look at qualitative data first. This type of data is categorical - it is used to categorize or identify the entity being observed. Sometimes you'll see features of this type described as *factors*. \n",
    "##### Nominal Data\n",
    "In his observations of children's height, Galton assigned an identifier to each family and he recorded the gender of each child. Note that even though the **family** identifier is a number, it is not a measurement or quantity. Family 002 it not \"greater\" than family 001, just as a **gender** value of \"male\" does not indicate a larger or smaller value than \"female\". These are simply named values for some characteristic of the child, and as such they're known as *nominal* data.\n",
    "##### Ordinal Data\n",
    "So what about the **childNum** feature? It's not a measurement or quantity - it's just a way to identify individual children within a family. However, the number assigned to each child has some additional meaning - the numbers are ordered. You can find similar data that is text-based; for example, data about training courses might include a \"level\" attribute that indicates the level of the course as \"basic:, \"intermediate\", or \"advanced\". This type of data, where the value is not itself a quantity or measurement, but it indicates some sort of inherent order or heirarchy, is known as *ordinal* data.\n",
    "#### Quantitative Data\n",
    "Now let's turn our attention to the features that indicate some kind of quantity or measurement.\n",
    "##### Discrete Data\n",
    "Galton's observations include the number of **children** in each family. This is a *discrete* quantative data value - it's something we *count* rather than *measure*. You can't, for example, have 2.33 children!\n",
    "##### Continuous Data\n",
    "The data set also includes height values for **father**, **mother**, **midparentHeight**, and **childHeight**. These are measurements along a scale, and as such they're described as *continuous* quantative data values that we *measure* rather than *count*.\n",
    "\n",
    "### Sample vs Population\n",
    "Galton's dataset includes 933 observations. It's safe to assume that this does not account for every person in the world, or even just the UK, in 1886 when the data was collected. In other words, Galton's data represents a *sample* of a larger *population*. It's worth pausing to think about this for a few seconds, because there are some implications for any conclusions we might draw from Galton's observations.\n",
    "\n",
    "Think about how many times you see a claim such as \"one in four Americans enjoys watching football\". How do the people who make this claim know that this is a fact? Have they asked everyone in the the US about their football-watching habits? Well, that would be a bit impractical, so what usually happens is that a study is conducted on a subset of the population, and (assuming that this is a well-conducted study), that subset will be a representative sample of the population as a whole. If the survey was conducted at the stadium where the Superbowl is being played, then the results are likely to be skewed because of a bias in the study participants.\n",
    "\n",
    "Similarly, we might look at Galton's data and assume that the heights of the people included in the study bears some relation to the heights of the general population in 1886; but if Galton specifically selected abnormally tall people for his study, then this assumption would be unfounded.\n",
    "\n",
    "When we deal with statistics, we usually work with a sample of the data rather than a full population. As you'll see later, this affects the way we use notation to indicate statistical measures; and in some cases we calculate statistics from a sample differently than from a full population to account for bias in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Data\n",
    "Data visualization is one of the key ways in which we can examine data and get insights from it. If a picture is worth a thousand words, then a good graph or chart is worth any number of tables of data.\n",
    "\n",
    "Let's examine some common kinds of data visualization:\n",
    "#### Bar Charts\n",
    "A *bar chart* is a good way to compare numeric quantities or counts across categories. For example, in the Galton dataset, you might want to compare the number of female and male children.\n",
    "\n",
    "Here's some Python code to create a bar chart showing the number of children of each gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "df = sm.datasets.get_rdataset('GaltonFamilies', package='HistData').data\n",
    "\n",
    "# Create a data frame of gender counts\n",
    "genderCounts = df['gender'].value_counts()\n",
    "\n",
    "# Plot a bar chart\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "genderCounts.plot(kind='bar', title='Gender Counts')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Number of Children')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this chart, you can see that there are slightly more male children than female children; but the data is reasonably evenly split between the two genders.\n",
    "\n",
    "Bar charts are typically used to compare categorical (qualitative) data values; but in some cases you might treat a discrete quantitative data value as a category. For example, in the Galton dataset the number of children in each family could be used as a way to categorize families. We might want to see how many familes have one child, compared to how many have two children, etc.\n",
    "\n",
    "Here's some Python code to create a bar chart showing family counts based on the number of children in the family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "df = sm.datasets.get_rdataset('GaltonFamilies', package='HistData').data\n",
    "\n",
    "# Create a data frame of child counts\n",
    "# there's a row for each child, so we need to filter to one row per family to avoid over-counting\n",
    "families = df[['family', 'children']].drop_duplicates()\n",
    "# Now count number of rows for each 'children' value, and sort by the index (children)\n",
    "childCounts = families['children'].value_counts().sort_index()\n",
    "\n",
    "\n",
    "# Plot a bar chart\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "childCounts.plot(kind='bar', title='Family Size')\n",
    "plt.xlabel('Number of Children')\n",
    "plt.ylabel('Families')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the code sorts the data so that the categories on the *x* axis are in order - attention to this sort of detail can make your charts easier to read. In this case, we can see that the most common number of children per family is 1, followed by 5 and 6. Comparatively fewer families have more than 8 children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms\n",
    "Bar charts work well for comparing categorical or discrete numeric values. When you need to compare continuous quantitative values, you can use a similar style of chart called a *histogram*. Histograms differ from bar charts in that they group the continuous values into ranges or *bins* - so the chart doesn't show a bar for each individual value, but rather a bar for each range of binned values. Because these bins represent continuous data rather than discrete data, the bars aren't separated by a gap. Typically, a histogram is used to show the relative frequency of values in the dataset.\n",
    "\n",
    "Here's some Python code to create a histogram of the **father** values in the Galton dataset, which record the father's height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "df = sm.datasets.get_rdataset('GaltonFamilies', package='HistData').data\n",
    "\n",
    "# Plot a histogram of midparentHeight\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df['father'].plot.hist(title='Father Heights')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows that the most frequently occuring heights tend to be in the mid-range. There are fewer extremely short or exteremely tall fathers.\n",
    "\n",
    "In the histogram above, the number of bins (and their corresponding ranges, or *bin widths*) was determined automatically by Python. In some cases you may want to explicitly control the number of bins, as this can help you see detail in the distribution of data values that otherwise you might miss. The following code creates a histogram for the same father's height values, but explicitly distributes them over 20 bins (19 are specified, and Python adds one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "df = sm.datasets.get_rdataset('GaltonFamilies', package='HistData').data\n",
    "\n",
    "# Plot a histogram of midparentHeight\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df['father'].plot.hist(title='Father Heights', bins=19)\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still see that the most common heights are in the middle, but there's a notable drop in the number of fathers with a height between 67.5 and 70."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie Charts\n",
    "Pie charts are another way to compare relative quantities of categories. They're not commonly used by data scientists, but they can be useful in many business contexts with manageable numbers of categories because they not only make it easy to compare relative quantities by categories; they also show those quantities as a proportion of the whole set of data.\n",
    "\n",
    "Here's some Python to show the gender counts as a pie chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "df = sm.datasets.get_rdataset('GaltonFamilies', package='HistData').data\n",
    "\n",
    "# Create a data frame of gender counts\n",
    "genderCounts = df['gender'].value_counts()\n",
    "\n",
    "# Plot a pie chart\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "genderCounts.plot(kind='pie', title='Gender Counts', figsize=(6,6))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the chart includes a *legend* to make it clear what category each colored area in the pie chart represents. From this chart, you can see that males make up slightly more than half of the overall number of children; with females accounting for the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plots\n",
    "Often you'll want to compare quantative values. This can be especially useful in data science scenarios where you are exploring data prior to building a machine learning model, as it can help identify apparent relationships between numeric features. Scatter plots can also help identify potential outliers - values that are significantly outside of the normal range of values.\n",
    "\n",
    "The following Python code creates a scatter plot that plots the intersection points for  **midparentHeight** on the *x* axis, and **childHeight** on the *y* axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "df = sm.datasets.get_rdataset('GaltonFamilies', package='HistData').data\n",
    "\n",
    "# Create a data frame of heights (father vs child)\n",
    "parentHeights = df[['midparentHeight', 'childHeight']]\n",
    "\n",
    "# Plot a scatter plot chart\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "parentHeights.plot(kind='scatter', title='Parent vs Child Heights', x='midparentHeight', y='childHeight')\n",
    "plt.xlabel('Avg Parent Height')\n",
    "plt.ylabel('Child Height')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a scatter plot, each dot marks the intersection point of the two values being plotted. In this chart, most of the heights are clustered around the center; which indicates that most parents and children tend to have a height that is somewhere in the middle of the range of heights observed. At the bottom left, there's a small cluster of dots that show some parents from the shorter end of the range who have children that are also shorter than their peers. At the top right, there are a few extremely tall parents who have extremely tall children. It's also interesting to note that the top left and bottom right of the chart are empty - there aren't any cases of extremely short parents with extremely tall children or vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line Charts\n",
    "*Line charts* are a great way to see changes in values along a series - usually (but not always) based on a time period. The Galton dataset doesn't include any data of this type, so we'll use a different dataset that includes observations of sea surface temperature between 1950 and 2010 for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "df = sm.datasets.elnino.load_pandas().data\n",
    "\n",
    "df['AVGSEATEMP'] = df.mean(1)\n",
    "\n",
    "# Plot a line chart\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "df.plot(title='Average Sea Temperature', x='YEAR', y='AVGSEATEMP')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Sea Temp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line chart shows the temperature trend from left to right for the period of observations. From this chart, you can see that the average temperature fluctuates from year to year, but the general trend shows an increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Fundamentals\n",
    "Statistics is primarily about analyzing data samples, and that starts with udnerstanding the distribution of data in a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Data Distribution\n",
    "A great deal of statistical analysis is based on the way that data values are distributed within the dataset. In this section, we'll explore some statistics that you can use to tell you about the values in a dataset.\n",
    "\n",
    "### Measures of Central Tendency\n",
    "The term *measures of central tendency* sounds a bit grand, but really it's just a fancy way of saying that we're interested in knowing where the middle value in our data is. For example, suppose decide to conduct a study into the comparative salaries of people who graduated from the same school. You might record the results like this:\n",
    "\n",
    "| Name     | Salary      |\n",
    "|----------|-------------|\n",
    "| Dan      | 50,000      |\n",
    "| Joann    | 54,000      |\n",
    "| Pedro    | 50,000      |\n",
    "| Rosie    | 189,000     |\n",
    "| Ethan    | 55,000      |\n",
    "| Vicky    | 40,000      |\n",
    "| Frederic | 59,000      |\n",
    "\n",
    "Now, some of the former-students may earn a lot, and others may earn less; but what's the salary in the middle of the range of all salaries?\n",
    "\n",
    "#### Mean\n",
    "A common way to define the central value is to use the *mean*, often called the *average*. This is calculated as the sum of the values in the dataset, divided by the number of observations in the dataset. When the dataset consists of the full population, the mean is represented by the Greek symbol ***&mu;*** (*mu*), and the formula is written like this:\n",
    "\n",
    "\\begin{equation}\\mu = \\frac{\\displaystyle\\sum_{i=1}^{N}X_{i}}{N}\\end{equation}\n",
    "\n",
    "More commonly, when working with a sample, the mean is represented by ***x&#772;*** (*x-bar*), and the formula is written like this (note the lower case letters used to indicate values from a sample):\n",
    "\n",
    "\\begin{equation}\\bar{x} = \\frac{\\displaystyle\\sum_{i=1}^{n}x_{i}}{n}\\end{equation}\n",
    "\n",
    "In the case of our list of heights, this can be calculated as:\n",
    "\n",
    "\\begin{equation}\\bar{x} = \\frac{50000+54000+50000+189000+55000+40000+59000}{7}\\end{equation}\n",
    "\n",
    "Which is **71,000**.\n",
    "\n",
    ">In technical terminology, ***x&#772;*** is a *statistic* (an estimate based on a sample of data) and ***&mu;*** is a *parameter* (a true value based on the entire population). A lot of the time, the parameters for the full population will be impossible (or at the very least, impractical) to measure; so we use statistics obtained from a representative sample to approximate them. In this case, we can use the sample mean of salary for our selection of surveyed students to try to estimate the actual average salary of all students who graduate from our school.\n",
    "\n",
    "In Python, when working with data in a *pandas.dataframe*, you can use the ***mean*** function, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000]})\n",
    "\n",
    "print (df['Salary'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, is **71,000** really the central value? Or put another way, would it be reasonable for a graduate of this school to expect to earn $71,000? After all, that's the average salary of a graduate from this school.\n",
    "\n",
    "If you look closely at the salaries, you can see that out of the seven former students, six earn less than the mean salary. The data is *skewed* by the fact that Rosie has clearly managed to find a much higher-paid job than her classmates.\n",
    "\n",
    "#### Median\n",
    "OK, let's see if we can find another definition for the central value that more closely reflects the expected earning potential of students attending our school. Another measure of central tendancy we can use is the *median*. To calculate the median, we need to sort the values into ascending order and then find the middle-most value. When there are an odd number of observations, you can find the position of the median value using this formula (where *n* is the number of observations):\n",
    "\n",
    "\\begin{equation}\\frac{n+1}{2}\\end{equation}\n",
    "\n",
    "Remember that this formula returns the *position* of the median value in the sorted list; not the value itself.\n",
    "\n",
    "If the number of observations is even, then things are a little (but not much) more complicated. In this case you calculate the median as the average of the two middle-most values, which are found like this:\n",
    "\n",
    "\\begin{equation}\\frac{n}{2} \\;\\;\\;\\;and \\;\\;\\;\\; \\frac{n}{2} + 1\\end{equation}\n",
    "\n",
    "So, for our graduate salaries; first lets sort the dataset:\n",
    "\n",
    "| Salary      |\n",
    "|-------------|\n",
    "| 40,000      |\n",
    "| 50,000      |\n",
    "| 50,000      |\n",
    "| 54,000      |\n",
    "| 55,000      |\n",
    "| 59,000      |\n",
    "| 189,000     |\n",
    "\n",
    "There's an odd number of observation (7), so the median value is at position (7 + 1) &div; 2; in other words, position 4:\n",
    "\n",
    "| Salary      |\n",
    "|-------------|\n",
    "| 40,000      |\n",
    "| 50,000      |\n",
    "| 50,000      |\n",
    "|***>54,000*** |\n",
    "| 55,000      |\n",
    "| 59,000      |\n",
    "| 189,000     |\n",
    "\n",
    "So the median salary is **54,000**.\n",
    "\n",
    "The *pandas.dataframe* class in Python has a ***median*** function to find the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000]})\n",
    "\n",
    "print (df['Salary'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode\n",
    "Another related statistic is the *mode*, which indicates the most frequently occurring value. If you think about it, this is potentially a good indicator of how much a student might expect to earn when they graduate from the school; out of all the salaries that are being earned by former students, the mode is earned by more than any other.\n",
    "\n",
    "Looking at our list of salaries, there are two instances of former students earning **50,000**, but only one instance each for all other salaries:\n",
    "\n",
    "| Salary      |\n",
    "|-------------|\n",
    "| 40,000      |\n",
    "|***>50,000***|\n",
    "|***>50,000***|\n",
    "| 54,000      |\n",
    "| 55,000      |\n",
    "| 59,000      |\n",
    "| 189,000     |\n",
    "\n",
    "The mode is therefore **50,000**.\n",
    "\n",
    "As you might expect, the *pandas.dataframe* class has a ***mode*** function to return the mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000]})\n",
    "\n",
    "print (df['Salary'].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multimodal Data\n",
    "It's not uncommon for a set of data to have more than one value as the mode. For example, suppose Ethan receives a raise that takes his salary to **59,000**:\n",
    "\n",
    "| Salary      |\n",
    "|-------------|\n",
    "| 40,000      |\n",
    "|***>50,000***|\n",
    "|***>50,000***|\n",
    "| 54,000      |\n",
    "|***>59,000***|\n",
    "|***>59,000***|\n",
    "| 189,000     |\n",
    "\n",
    "Now there are two values with the highest frequency. This dataset is *bimodal*. More generally, when there is more than one mode value, the data is considered *multimodal*.\n",
    "\n",
    "The *pandas.dataframe.**mode*** function returns all of the modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,59000,40000,59000]})\n",
    "\n",
    "print (df['Salary'].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution and Density\n",
    "Now we know something about finding the center, we can start to explore how the data is distributed around it. What we're interested in here is understanding the general \"shape\" of the data distribution so that we can begin to get a feel for what a 'typical' value might be expected to be.\n",
    "\n",
    "We can start by finding the extremes - the minimum and maximum. In the case of our salary data, the lowest paid graduate from our school is Vicky, with a salary of **40,000**; and the highest-paid graduate is Rosie, with **189,000**.\n",
    "\n",
    "The *pandas.dataframe* class has ***min*** and ***max*** functions to return these values.\n",
    "\n",
    "Run the following code to compare the minimum and maximum salaries to the central measures we calculated previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000]})\n",
    "\n",
    "print ('Min: ' + str(df['Salary'].min()))\n",
    "print ('Mode: ' + str(df['Salary'].mode()[0]))\n",
    "print ('Median: ' + str(df['Salary'].median()))\n",
    "print ('Mean: ' + str(df['Salary'].mean()))\n",
    "print ('Max: ' + str(df['Salary'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine these values, and get a sense for how the data is distributed - for example, we can see that the *mean* is closer to the max than the *median*, and that both are closer to the *min* than to the *max*.\n",
    "\n",
    "However, it's generally easier to get a sense of the distribution by visualizing the data. Let's start by creating a histogram of the salaries, highlighting the *mean* and *median* salaries (the *min*, *max* are fairly self-evident, and the *mode* is wherever the highest bar is):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000]})\n",
    "\n",
    "salary = df['Salary']\n",
    "salary.plot.hist(title='Salary Distribution', color='lightblue', bins=25)  \n",
    "plt.axvline(salary.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(salary.median(), color='green', linestyle='dashed', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <span style=\"color:magenta\">***mean***</span> and <span style=\"color:green\">***median***</span> are shown as dashed lines. Note the following:\n",
    "- *Salary* is a continuous data value - graduates could potentially earn any value along the scale, even down to a fraction of cent.\n",
    "- The number of bins in the histogram determines the size of each salary band for which we're counting frequencies. Fewer bins means merging more individual salaries together to be counted as a group.\n",
    "- The majority of the data is on the left side of the histogram, reflecting the fact that most graduates earn between 40,000 and 55,000\n",
    "- The mean is a higher value than the median and mode.\n",
    "- There are gaps in the histogram for salary bands that nobody earns.\n",
    "\n",
    "The histogram shows the relative frequency of each salary band, based on the number of bins. It also gives us a sense of the *density* of the data for each point on the salary scale. With enough data points, and small enough bins, we could view this density as a line that shows the shape of the data distribution.\n",
    "\n",
    "Run the following cell to show the density of the salary data as a line on top of the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000]})\n",
    "\n",
    "salary = df['Salary']\n",
    "density = stats.gaussian_kde(salary)\n",
    "n, x, _ = plt.hist(salary, histtype='step', normed=True, bins=25)  \n",
    "plt.plot(x, density(x)*5)\n",
    "plt.axvline(salary.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(salary.median(), color='green', linestyle='dashed', linewidth=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the density line takes the form of an asymmetric curve that has a \"peak\" on the left and a long tail on the right. We describe this sort of data distribution as being *skewed*; that is, the data is not distributed symmetrically but \"bunched together\" on one side. In this case, the data is bunched together on the left, creating a long tail on the right; and is described as being *right-skewed* because some infrequently occurring high values are pulling the *mean* to the right.\n",
    "\n",
    "Let's take a look at another set of data. We know how much money our graduates make, but how many hours per week do they need to work to earn their salaries? Here's the data:\n",
    "\n",
    "| Name     | Hours |\n",
    "|----------|-------|\n",
    "| Dan      | 41    |\n",
    "| Joann    | 40    |\n",
    "| Pedro    | 36    |\n",
    "| Rosie    | 30    |\n",
    "| Ethan    | 35    |\n",
    "| Vicky    | 39    |\n",
    "| Frederic | 40    |\n",
    "\n",
    "Run the following code to show the distribution of the hours worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Hours':[41,40,36,30,35,39,40]})\n",
    "\n",
    "hours = df['Hours']\n",
    "density = stats.gaussian_kde(hours)\n",
    "n, x, _ = plt.hist(hours, histtype='step', normed=True, bins=25)  \n",
    "plt.plot(x, density(x)*7)\n",
    "plt.axvline(hours.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(hours.median(), color='green', linestyle='dashed', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the distribution is skewed, but this time it's **left-skewed**. Note that the curve is asymmetric with the <span style=\"color:magenta\">***mean***</span> to the left of the <span style=\"color:green\">***median***</span> and the *mode*; and the average weekly working hours skewed to the lower end.\n",
    "\n",
    "Once again, Rosie seems to be getting the better of the deal. She earns more than her former classmates for working fewer hours. Maybe a look at the test scores the students achieved on their final grade at school might help explain her success:\n",
    "\n",
    "| Name     | Grade |\n",
    "|----------|-------|\n",
    "| Dan      | 50    |\n",
    "| Joann    | 50    |\n",
    "| Pedro    | 46    |\n",
    "| Rosie    | 95    |\n",
    "| Ethan    | 50    |\n",
    "| Vicky    | 5     |\n",
    "| Frederic | 57    |\n",
    "\n",
    "Let's take a look at the distribution of these grades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "\n",
    "grade = df['Grade']\n",
    "density = stats.gaussian_kde(grade)\n",
    "n, x, _ = plt.hist(grade, histtype='step', normed=True, bins=25)  \n",
    "plt.plot(x, density(x)*7.5)\n",
    "plt.axvline(grade.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(grade.median(), color='green', linestyle='dashed', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the distribution is symmetric, forming a \"bell-shaped\" curve. The <span style=\"color:magenta\">***mean***</span>, <span style=\"color:green\">***median***</span>, and mode are at the same location, and the data tails off evenly on both sides from a central peak.\n",
    "\n",
    "Statisticians call this a *normal* distribution (or sometimes a *Gaussian* distribution), and it occurs quite commonly in many scenarios due to something called the *Central Limit Theorem*, which reflects the way continuous probability works - more about that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skewness and Kurtosis\n",
    "You can measure *skewness* (in which direction the data is skewed and to what degree) and kurtosis (how \"peaked\" the data is) to get an idea of the shape of the data distribution. In Python, you can use the ***skew*** and ***kurt*** functions to find this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,30,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "\n",
    "numcols = ['Salary', 'Hours', 'Grade']\n",
    "for col in numcols:\n",
    "    print(df[col].name + ' skewness: ' + str(df[col].skew()))\n",
    "    print(df[col].name + ' kurtosis: ' + str(df[col].kurt()))\n",
    "    density = stats.gaussian_kde(df[col])\n",
    "    n, x, _ = plt.hist(df[col], histtype='step', normed=True, bins=25)  \n",
    "    plt.plot(x, density(x)*6)\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the distribution of a real dataset - let's see how the heights of the father's measured in Galton's study of parent and child heights are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df = sm.datasets.get_rdataset('GaltonFamilies', package='HistData').data\n",
    "\n",
    "\n",
    "fathers = df['father']\n",
    "density = stats.gaussian_kde(fathers)\n",
    "n, x, _ = plt.hist(fathers, histtype='step', normed=True, bins=50)  \n",
    "plt.plot(x, density(x)*2.5)\n",
    "plt.axvline(fathers.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(fathers.median(), color='green', linestyle='dashed', linewidth=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the father's height measurements are approximately normally distributed - in other words, they form a more or less *normal* distribution that is symmetric around the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Variance\n",
    "We can see from the distribution plots of our data that the values in our dataset can vary quite widely. We can use various measures to quantify this variance.\n",
    "\n",
    "#### Range\n",
    "A simple way to quantify the variance in a dataset is to identify the difference between the lowest and highest values. This is called the *range*, and is calculated by subtracting the minimim value from the maximum value.\n",
    "\n",
    "The following Python code creates a single Pandas dataframe for our school graduate data, and calculates the *range* for each of the numeric features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,30,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "\n",
    "numcols = ['Salary', 'Hours', 'Grade']\n",
    "for col in numcols:\n",
    "    print(df[col].name + ' range: ' + str(df[col].max() - df[col].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentiles and Quartiles\n",
    "The range is easy to calculate, but it's not a particularly useful statistic. For example, a range of 149,000 between the lowest and highest salary does not tell us which value within that range a graduate is most likely to earn -  it doesn't tell us nothing about how the salaries are distributed around the mean within that range.  The range tells us very little about the comparative position of an individual value within the distribution - for example, Frederic scored 57 in his final grade at school; which is a pretty good score (it's more than all but one of his classmates); but this isn't immediately apparent from a score of 57 and range of 90.\n",
    "\n",
    "##### Percentiles\n",
    "A percentile tells us where a given value is ranked in the overall distribution. For example, 25% of the data in a distribution has a value lower than the 25th percentile; 75% of the data has a value lower than the 75th percentile, and so on. Note that half of the data has a value lower than the 50th percentile - so the 50th percentile is also the median!\n",
    "\n",
    "Let's examine Frederic's grade using this approach. We know he scored 57, but how does he rank compared to his fellow students?\n",
    "\n",
    "Well, there are seven students in total, and five of them scored less than Frederic; so we can calculate the percentile for Frederic's grade like this:\n",
    "\n",
    "\\begin{equation}\\frac{5}{7} \\times 100 \\approx 71.4\\end{equation} \n",
    "\n",
    "So Frederic's score puts him at the 71.4th percentile in his class.\n",
    "\n",
    "In Python, you can use the ***percentileofscore*** function in the *scipy.stats* package to calculate the percentile for a given value in a set of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,30,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "\n",
    "print(stats.percentileofscore(df['Grade'], 57, 'strict'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used the strict definition of percentile; but sometimes it's calculated as being the percentage of values that are less than *or equal to* the value you're comparing. In this case, the calculation for Frederic's percentile would include his own score:\n",
    "\n",
    "\\begin{equation}\\frac{6}{7} \\times 100 \\approx 85.7\\end{equation} \n",
    "\n",
    "You can calculate this way in Python by using the ***weak*** mode of the ***percentileofscore*** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,30,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "\n",
    "print(stats.percentileofscore(df['Grade'], 57, 'weak'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've considered the percentile of Frederic's grade, and used it to rank him compared to his fellow students. So what about Dan, Joann, and Ethan? How do they compare to the rest of the class? They scored the same grade (50), so in a sense they share a percentile.\n",
    "\n",
    "To deal with this *grouped* scenario, we can average the percentage rankings for the matching scores. We treat half of the scores matching the one we're ranking as if they are below it, and half as if they are above it. In this case, there were three matching scores of 50, and for each of these we calculate the percentile as if 1 was below and 1 was above. So the calculation for a percentile for Joann based on scores being less than or equal to 50 is:\n",
    "\n",
    "\\begin{equation}(\\frac{4}{7}) \\times 100 \\approx 57.14\\end{equation} \n",
    "\n",
    "The value of **4** consists of the two scores that are below Joann's score of 50, Joann's own score, and half of the scores that are the same as Joann's (of which there are two, so we count one).\n",
    "\n",
    "In Python, the ***percentileofscore*** function has a ***rank*** function that calculates grouped percentiles like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,30,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "\n",
    "print(stats.percentileofscore(df['Grade'], 50, 'rank'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quartiles\n",
    "Rather than using individual percentiles to compare data, we can consider the overall spread of the data by dividing those percentiles into four *quartiles*. The first quartile contains the values from the minimum to the 25th percentile, the second from the 25th percentile to the 50th percentile (which is the median), the third from the 50th percentile to the 75th percentile, and the fourth from the 75th percentile to the maximum.\n",
    "\n",
    "In Python, you can use the ***quantile*** function of the *pandas.dataframe* class to find the threshold values at the 25th, 50th, and 75th percentiles (*quantile* is a generic term for a ranked position, such as a percentile or quartile).\n",
    "\n",
    "Run the following code to find the quartile thresholds for the weekly hours worked by our former students:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quartiles\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,17,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "print(df['Hours'].quantile([0.25, 0.5, 0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its usually easier to understand how data is distributed across the quartiles by visualizing it. You can use a histogram, but many data scientists use a kind of visualization called a *box plot* (or a *box and whiskers* plot).\n",
    "\n",
    "Let's create a box plot for the weekly hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,30,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "\n",
    "# Plot a box-whisker chart\n",
    "df['Hours'].plot(kind='box', title='Weekly Hours Distribution', figsize=(10,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plot consists of:\n",
    "- A rectangular *box* that shows where the data between the 25th and 75th percentile (the second and third quartile) lie. This part of the distribution is often referred to as the *interquartile range* - it contains the middle 50 data values.\n",
    "- *Whiskers* that extend from the box to the bottom of the first quartile and the top of the fourth quartile to show the full range of the data.\n",
    "- A line in the box that shows that location of the median (the 50th percentile, which is also the threshold between the second and third quartile)\n",
    "\n",
    "In this case, you can see that the interquartile range is between 35 and 40, with the median nearer the top of that range. The range of the first quartile is from around 30 to 35, and the fourth quartile is from 40 to 41."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "Let's take a look at another box plot - this time showing the distribution of the salaries earned by our former classmates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,30,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "\n",
    "# Plot a box-whisker chart\n",
    "df['Salary'].plot(kind='box', title='Salary Distribution', figsize=(10,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's going on here?\n",
    "\n",
    "Well, as we've already noticed, Rosie earns significantly more than her former classmates. So much more in fact, that her salary has been identifed as an *outlier*. An outlier is a value that is so far from the center of the distribution compared to other values that it skews the distribution by affecting the mean. There are all sorts of reasons that you might have outliers in your data, including data entry errors, failures in sensors or data-generating equipment, or genuinely anomalous values.\n",
    "\n",
    "So what should we do about it?\n",
    "\n",
    "This really depends on the data, and what you're trying to use it for. In this case, let's assume we're trying to figure out what's a reasonable expectation of salary for a graduate of our school to earn. Ignoring for the moment that we have an extremly small dataset on which to base our judgement, it looks as if Rosie's salary could be either an error (maybe she mis-typed it in the form used to collect data) or a genuine anomaly (maybe she became a professional athelete or some other extremely highly paid job). Either way, it doesn't seem to represent a salary that a typical graduate might earn.\n",
    "\n",
    "Let's see what the distribution of the data looks like without the outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,17,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "\n",
    "# Plot a box-whisker chart\n",
    "df['Salary'].plot(kind='box', title='Salary Distribution', figsize=(10,8), showfliers=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it looks like there's a more even distribution of salaries. It's still not quite symmetrical, but there's much less overall variance. There's potentially some cause here to disregard Rosie's salary data when we compare the salaries, as it is tending to skew the analysis.\n",
    "\n",
    "So is that OK? Can we really just ignore a data value we don't like?\n",
    "\n",
    "Again, it depends on what you're analyzing. Let's take a look at the distribution of final grades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,17,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "\n",
    "# Plot a box-whisker chart\n",
    "df['Grade'].plot(kind='box', title='Grade Distribution', figsize=(10,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again there are outliers, this time at both ends of the distribution. However, think about what this data represents. If we assume that the grade for the final test is based on a score out of 100, it seems reasonable to expect that some students will score very low (maybe even 0) and some will score very well (maybe even 100); but most will get a score somewhere in the middle.  The reason that the low and high scores here look like outliers might just be because we have so few data points. Let's see what happens if we include a few more students in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n",
    "                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n",
    "\n",
    "# Plot a box-whisker chart\n",
    "df['Grade'].plot(kind='box', title='Grade Distribution', figsize=(10,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more data, there are some more high and low scores; so we no longer consider the isolated cases to be outliers.\n",
    "\n",
    "The key point to take away here is that you need to really understand the data and what you're trying to do with it, and you need to ensure that you have a reasonable sample size, before determining what to do with outlier values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance and Standard Deviation\n",
    "We've seen how to understand the *spread* of our data distribution using the range, percentiles, and quartiles; and we've seen the effect of outliers on the distribution. Now it's time to look at how to measure the amount of variance in the data.\n",
    "\n",
    "##### Variance\n",
    "Variance is measured as the average of the squared difference from the mean. For a full population, it's indicated by a squared Greek letter *sigma* (***&sigma;<sup>2</sup>***) and calculated like this:\n",
    "\n",
    "\\begin{equation}\\sigma^{2} = \\frac{\\displaystyle\\sum_{i=1}^{N} (X_{i} -\\mu)^{2}}{N}\\end{equation}\n",
    "\n",
    "For a sample, it's indicated as ***s<sup>2</sup>*** calculated like this:\n",
    "\n",
    "\\begin{equation}s^{2} = \\frac{\\displaystyle\\sum_{i=1}^{n} (x_{i} -\\bar{x})^{2}}{n-1}\\end{equation}\n",
    "\n",
    "In both cases, we sum the difference between the individual data values and the mean and square the result. Then, for a full population we just divide by the number of data items to get the average. When using a sample, we divide by the total number of items **minus 1** to correct for sample bias.\n",
    "\n",
    "Let's work this out for our student grades (assuming our data is a sample from the larger student population).\n",
    "\n",
    "First, we need to calculate the mean grade:\n",
    "\n",
    "\\begin{equation}\\bar{x} = \\frac{50+50+46+95+50+5+57}{7}\\approx 50.43\\end{equation}\n",
    "\n",
    "Then we can plug that into our formula for the variance:\n",
    "\n",
    "\\begin{equation}s^{2} = \\frac{(50-50.43)^{2}+(50-50.43)^{2}+(46-50.43)^{2}+(95-50.43)^{2}+(50-50.43)^{2}+(5-50.43)^{2}+(57-50.43)^{2}}{7-1}\\end{equation}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{equation}s^{2} = \\frac{0.185+0.185+19.625+1986.485+0.185+2063.885+43.165}{6}\\end{equation}\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "\\begin{equation}s^{2} = \\frac{4113.715}{6}\\end{equation}\n",
    "\n",
    "Giving the result:\n",
    "\n",
    "\\begin{equation}s^{2} \\approx 685.619\\end{equation}\n",
    "\n",
    "The higher the variance, the more spread your data is around the mean.\n",
    "\n",
    "In Python, you can use the ***var*** function of the *pandas.dataframe* class to calculate the variance of a column in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,17,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "\n",
    "print(df['Grade'].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standard Deviation\n",
    "To calculate the variance, we squared the difference of each value from the mean. If we hadn't done this, the numerator of our fraction would always end up being zero (because the mean is at the center of our values). However, this means that the variance is not in the same unit of measurement as our data - in our case, since we're calculating the variance for grade points, it's in grade points squared; which is not very helpful.\n",
    "\n",
    "To get the measure of variance back into the same unit of measurement, we need to find its square root:\n",
    "\n",
    "\\begin{equation}s = \\sqrt{685.619} \\approx 26.184\\end{equation}\n",
    "\n",
    "So what does this value represent?\n",
    "\n",
    "It's the *standard deviation* for our grades data. More formally, it's calculated like this for a full population:\n",
    "\n",
    "\\begin{equation}\\sigma = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^{N} (X_{i} -\\mu)^{2}}{N}}\\end{equation}\n",
    "\n",
    "Or like this for a sample:\n",
    "\n",
    "\\begin{equation}s = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^{n} (x_{i} -\\bar{x})^{2}}{n-1}}\\end{equation}\n",
    "\n",
    "Note that in both cases, it's just the square root of the corresponding variance forumla!\n",
    "\n",
    "In Python, you can calculate it using the ***std*** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,17,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "\n",
    "print(df['Grade'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Deviation in a Normal Distribution\n",
    "\n",
    "In statistics and data science, we spend a lot of time considering *normal* distributions; because they occur so frequently. The standard deviation has an important relationship to play in a normal distribution.\n",
    "\n",
    "Run the following cell to show a histogram of a *standard normal* distribution (which is a distribution with a mean of 0 and a standard deviation of 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Create a random standard normal distribution\n",
    "df = pd.DataFrame(np.random.randn(100000, 1), columns=['Grade'])\n",
    "\n",
    "# Plot the distribution as a histogram with a density curve\n",
    "grade = df['Grade']\n",
    "density = stats.gaussian_kde(grade)\n",
    "n, x, _ = plt.hist(grade, color='lightgrey', normed=True, bins=100)  \n",
    "plt.plot(x, density(x))\n",
    "\n",
    "# Get the mean and standard deviation\n",
    "s = df['Grade'].std()\n",
    "m = df['Grade'].mean()\n",
    "\n",
    "# Annotate 1 stdev\n",
    "x1 = [m-s, m+s]\n",
    "y1 = [0.25, 0.25]\n",
    "plt.plot(x1,y1, color='magenta')\n",
    "plt.annotate('1s (68.26%)', (x1[1],y1[1]))\n",
    "\n",
    "# Annotate 2 stdevs\n",
    "x2 = [m-(s*2), m+(s*2)]\n",
    "y2 = [0.05, 0.05]\n",
    "plt.plot(x2,y2, color='green')\n",
    "plt.annotate('2s (95.45%)', (x2[1],y2[1]))\n",
    "\n",
    "# Annotate 3 stdevs\n",
    "x3 = [m-(s*3), m+(s*3)]\n",
    "y3 = [0.005, 0.005]\n",
    "plt.plot(x3,y3, color='orange')\n",
    "plt.annotate('3s (99.73%)', (x3[1],y3[1]))\n",
    "\n",
    "# Show the location of the mean\n",
    "plt.axvline(grade.mean(), color='grey', linestyle='dashed', linewidth=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The horizontal colored lines show the percentage of data within 1, 2, and 3 standard deviations of the mean (plus or minus).\n",
    "\n",
    "In any normal distribution:\n",
    "- Approximately 68.26% of values fall within one standard deviation from the mean.\n",
    "- Approximately 95.45% of values fall within two standard deviations from the mean.\n",
    "- Approximately 99.73% of values fall within three standard deviations from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z Score\n",
    "So in a normal (or close to normal) distribution, standard deviation provides a way to evaluate how far from a mean a given range of values falls, allowing us to compare where a particular value lies within the distribution. For example, suppose Rosie tells you she was the highest scoring student among her friends - that doesn't really help us assess how well she scored. She may have scored only a fraction of a point above the second-highest scoring student. Even if we know she was in the top quartile; if we don't know how the rest of the grades are distributed it's still not clear how well she performed compared to her friends.\n",
    "\n",
    "However, if she tells you how many standard deviations higher than the mean her score was, this will help you compare her score to that of her classmates.\n",
    "\n",
    "So how do we know how many standard deviations above or below the mean a particular value is? We call this a *Z Score*, and it's calculated like this for a full population:\n",
    "\n",
    "\\begin{equation}Z = \\frac{x - \\mu}{\\sigma}\\end{equation}\n",
    "\n",
    "or like this for a sample:\n",
    "\n",
    "\\begin{equation}Z = \\frac{x - \\bar{x}}{s}\\end{equation}\n",
    "\n",
    "So, let's examine Rosie's grade of 95. Now that we know the *mean* grade is 50.43 and the *standard deviation* is 26.184, we can calculate the Z Score for this grade like this:\n",
    "\n",
    "\\begin{equation}Z = \\frac{95 - 50.43}{26.184} = 1.702\\end{equation}.\n",
    "\n",
    "So Rosie's grade is 1.702 standard deviations above the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing Data Distribution in Python\n",
    "We've seen how to obtain individual statistics in Python, but you can also use the ***describe*** function to retrieve summary statistics for all numeric columns in a dataframe. These summary statistics include many of the statistics we've examined so far (though it's worth noting that the *median* is not included):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,17,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Data\n",
    "You'll often want to compare data in your dataset, to see if you can discern trends or relationships.\n",
    "\n",
    "## Univariate Data\n",
    "*Univariate* data is data that consist of only one variable or feature. While it may initially seem as though there's not much we can do to analyze univariate data, we've already seen that we can explore its distribution in terms of measures of central tendency and measures of variance. We've also seen how we can visualize this distribution using histograms and box plots.\n",
    "\n",
    "Here's a reminder of how you can visualize the distribution of univariate data, using our student grade data with a few additional observations in the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n",
    "                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n",
    "\n",
    "plt.figure()\n",
    "df['Grade'].plot( kind='box', title='Grade Distribution')\n",
    "plt.figure()\n",
    "df['Grade'].hist(bins=9)\n",
    "plt.show()\n",
    "print(df.describe())\n",
    "print('median: ' + str(df['Grade'].median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate and Multivariate Data\n",
    "It can often be useful to compare *bivariate* data; in other words, compare two variables, or even more (in which case we call it *multivariate* data).\n",
    "\n",
    "For example, our student data includes three numeric variables for each student: their salary, the number of hours they work per week, and their final school grade. Run the following code to see an enlarged sample of this data as a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n",
    "                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n",
    "                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n",
    "\n",
    "df[['Name', 'Salary', 'Hours', 'Grade']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose you want to compare the distributions of these variables. You might simply create a boxplot for each variable, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n",
    "                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n",
    "                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n",
    "\n",
    "\n",
    "df.plot(kind='box', title='Distribution', figsize = (10,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that's not particularly useful is it?\n",
    "\n",
    "The problem is that the data are all measured in different scales. Salaries are typically in tens of thousands, while hours and grades are in single or double digits.\n",
    "\n",
    "### Normalizing Data\n",
    "When you need to compare data in different units of measurement, you can *normalize* or *scale* the data so that the values are measured in the same proportional scale. For example, in Python you can use a MinMax scaler to normalize multiple numeric variables to a proportional value between 0 and 1 based on their minimum and maximum values. Run the following cell to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n",
    "                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n",
    "                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "df[['Salary', 'Hours', 'Grade']] = scaler.fit_transform(df[['Salary', 'Hours', 'Grade']])\n",
    "\n",
    "# Plot the normalized data\n",
    "df.plot(kind='box', title='Distribution', figsize = (10,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the numbers on the y axis aren't particularly meaningful, but they're on a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Bivariate Data with a Scatter Plot\n",
    "When you need to compare two numeric values, a scatter plot can be a great way to see if there is any apparent relationship between them so that changes in the value of one variable affect the value of the other.\n",
    "\n",
    "Let's look at a scatter plot of *Salary* and *Grade*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n",
    "                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n",
    "                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n",
    "\n",
    "# Create a scatter plot of Salary vs Grade\n",
    "df.plot(kind='scatter', title='Grade vs Hours', x='Grade', y='Salary')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look closely at the scatter plot. Can you see a diagonal trend in the plotted points, rising up to the right? It looks as though the higher the student's grade is, the higher their salary is.\n",
    "\n",
    "You can see the trend more clearly by adding a *line of best fit* (sometimes called a *trendline*) to the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n",
    "                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n",
    "                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n",
    "\n",
    "# Create a scatter plot of Salary vs Grade\n",
    "df.plot(kind='scatter', title='Grade vs Salary', x='Grade', y='Salary')\n",
    "\n",
    "# Add a line of best fit\n",
    "plt.plot(np.unique(df['Grade']), np.poly1d(np.polyfit(df['Grade'], df['Salary'], 1))(np.unique(df['Grade'])))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line of best fit makes it clearer that there is some apparent *colinearity* between these variables (the relationship is *colinear* if one variable's value increases or decreases in line with the other).\n",
    "\n",
    "### Correlation\n",
    "The apparently colinear relationship you saw in the scatter plot can be verified by calculating a statistic that quantifies the relationship between the two variables. The statistic usually used to do this is *correlation*, though there is also a statistic named *covariance* that is sometimes used. Correlation is generally preferred because the value it produces is more easily interpreted.\n",
    "\n",
    "A correlation value is always a number between ***-1*** and ***1***.\n",
    "- A positive value indicates a positive correlation (as the value of variable *x* increases, so does the value of variable *y*).\n",
    "- A negative value indicates a negative correlation (as the value of variable *x* increases, the value of variable *y* decreases).\n",
    "- The closer to zero the correlation value is, the weaker the correlation between *x* and *y*.\n",
    "- A correlation of exactly zero means there is no apparent relationship between the variables.\n",
    "\n",
    "The formula to calculate correlation is:\n",
    "\n",
    "\\begin{equation}r_{x,y} = \\frac{\\displaystyle\\sum_{i=1}^{n} (x_{i} -\\bar{x})(y_{i} -\\bar{y})}{\\sqrt{\\displaystyle\\sum_{i=1}^{n} (x_{i} -\\bar{x})^{2}(y_{i} -\\bar{y})^{2}}}\\end{equation}\n",
    "\n",
    "**r<sub>x, y</sub>** is the notation for the *correlation between x and y*.\n",
    "\n",
    "The formula is pretty complex, but fortunately Python makes it very easy to calculate the correlation by using the ***corr*** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000],\n",
    "                   'Hours':[41,40,36,17,35,39,40],\n",
    "                   'Grade':[50,50,46,95,50,5,57]})\n",
    "\n",
    "# Calculate the correlation between *Salary* and *Grade*\n",
    "print(df['Grade'].corr(df['Salary']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the correlation is just over 0.8; making it a reasonably high positive correlation that indicates salary increases in line with grade.\n",
    "\n",
    "Let's see if we can find a correlation between *Grade* and *Hours*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n",
    "                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n",
    "                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n",
    "\n",
    "r = df['Grade'].corr(df['Hours'])\n",
    "print('Correlation: ' + str(r))\n",
    "\n",
    "# Create a scatter plot of Salary vs Grade\n",
    "df.plot(kind='scatter', title='Grade vs Hours', x='Grade', y='Hours')\n",
    "\n",
    "# Add a line of best fit-\n",
    "plt.plot(np.unique(df['Grade']), np.poly1d(np.polyfit(df['Grade'], df['Hours'], 1))(np.unique(df['Grade'])))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the correlation value is just under -0.8; meaning a fairly strong negative correlation in which the number of hours worked decreases as the grade increases. The line of best fit on the scatter plot corroborates this statistic.\n",
    "\n",
    "It's important to remember that *correlation* **is not** *causation*. In other words, even though there's an apparent relationship, you can't say for sure that one variable is the cause of the other. In this example, we can say that students who achieved higher grades tend to work shorter hours; but we ***can't*** say that those who work shorter hours do so *because* they achieved a high grade!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Regression\n",
    "In the previous examples, we drew a line on a scatter plot to show the *best fit* of the data. In many cases, your initial attempts to identify any colinearity might involve adding this kind of line by hand (or just mentally visualizing it); but as you may suspect from the use of the *numpy.**polyfit*** function in the code above, there are ways to calculate the coordinates for this line mathematically. One of the most commonly used techniques is *least squares regression*, and that's what we'll look at now.\n",
    "\n",
    "Cast your mind back to when you were learning how to solve linear equations, and recall that the *slope-intercept* form of a linear equation lookes like this:\n",
    "\n",
    "\\begin{equation}y = mx + b\\end{equation}\n",
    "\n",
    "In this equation, *y* and *x* are the coordinate variables, *m* is the slope of the line, and *b* is the y-intercept of the line.\n",
    "\n",
    "In the case of our scatter plot for our former-student's working hours, we already have our values for *x* (*Grade*) and *y* (*Hours*), so we just need to calculate the intercept and slope of the straight line that lies closest to those points. Then we can form a linear equation that calculates the a new *y* value on that line for each of our *x* (*Grade*) values - to avoid confusion, we'll call this new *y* value *f(x)* (because it's the output from a linear equation function based on *x*). The difference between the original *y* (*Hours*) value and the *f(x)* value is the *error* between our regression line of best fit and the actual *Hours* worked by the former student. Our goal is to calculate the slope and intercept for a line with the lowest overall error.\n",
    "\n",
    "Specifically, we define the overall error by taking the error for each point, squaring it, and adding all the squared errors together. The line of best fit is the line that gives us the lowest value for the sum of the squared errors - hence the name *least squares regression*.\n",
    "\n",
    "So how do we accomplish this? First we need to calculate the slope (*m*), which we do using this formula (in which *n* is the number of observations in our data sample):\n",
    "\n",
    "\\begin{equation}m = \\frac{n(\\sum{xy}) - (\\sum{x})(\\sum{y})}{n(\\sum{x^{2}})-(\\sum{x})^{2}}\\end{equation}\n",
    "\n",
    "After we've calculated the slope (*m*), we can use is to calculate the intercept (*b*) like this:\n",
    "\n",
    "\\begin{equation}b = \\frac{\\sum{y} - m(\\sum{x})}{n}\\end{equation}\n",
    "\n",
    "Let's look at a simple example that compares the number of hours of nightly study each student undertook with the final grade the student achieved:\n",
    "\n",
    "| Name     | Study | Grade |\n",
    "|----------|-------|-------|\n",
    "| Dan      | 1     | 50    |\n",
    "| Joann    | 0.75  | 50    |\n",
    "| Pedro    | 0.6   | 46    |\n",
    "| Rosie    | 2     | 95    |\n",
    "| Ethan    | 1     | 50    |\n",
    "| Vicky    | 0.2   | 5     |\n",
    "| Frederic | 1.2   | 57    |\n",
    "\n",
    "First, let's take each *x* (Study) and *y* (Grade) pair and calculate *x<sup>2</sup>* and *xy*, because we're going to need these to work out the slope:\n",
    "\n",
    "| Name     | Study | Grade | x<sup>2</sup> | xy   |\n",
    "|----------|-------|-------|------|------|\n",
    "| Dan      | 1     | 50    | 1    | 50   |\n",
    "| Joann    | 0.75  | 50    | 0.55 | 37.5 |\n",
    "| Pedro    | 0.6   | 46    | 0.36 | 27.6 |\n",
    "| Rosie    | 2     | 95    | 4    | 190  |\n",
    "| Ethan    | 1     | 50    | 1    | 50   |\n",
    "| Vicky    | 0.2   | 5     | 0.04 | 1    |\n",
    "| Frederic | 1.2   | 57    | 1.44 | 68.4 |\n",
    "\n",
    "Now we'll sum *x*, *y*, *x<sup>2</sup>*, and *xy*:\n",
    "\n",
    "| Name     | Study | Grade | x<sup>2</sup> | xy   |\n",
    "|----------|-------|-------|------|------|\n",
    "| Dan      | 1     | 50    | 1    | 50   |\n",
    "| Joann    | 0.75  | 50    | 0.55 | 37.5 |\n",
    "| Pedro    | 0.6   | 46    | 0.36 | 27.6 |\n",
    "| Rosie    | 2     | 95    | 4    | 190  |\n",
    "| Ethan    | 1     | 50    | 1    | 50   |\n",
    "| Vicky    | 0.2   | 5     | 0.04 | 1    |\n",
    "| Frederic | 1.2   | 57    | 1.44 | 68.4 |\n",
    "| **&Sigma;**      | **6.75**  | **353**   | **8.4025**| **424.5**  |\n",
    "\n",
    "OK, now we're ready to calculate the slope for our *7* observations:\n",
    "\n",
    "\\begin{equation}m = \\frac{(7\\times 424.5) - (6.75\\times353)}{(7\\times8.4025)-6.75^{2}}\\end{equation}\n",
    "\n",
    "Which is:\n",
    "\n",
    "\\begin{equation}m = \\frac{2971.5 - 2382.75}{58.8175-45.5625}\\end{equation}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{equation}m = \\frac{588.75}{13.255} \\approx 44.4172\\end{equation}\n",
    "\n",
    "Now we can calculate *b*:\n",
    "\n",
    "\\begin{equation}b = \\frac{353 - (44.4172\\times6.75)}{7}\\end{equation}\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "\\begin{equation}b = \\frac{53.18389}{7} = 7.597699\\end{equation}\n",
    "\n",
    "Now we have our linear function:\n",
    "\n",
    "\\begin{equation}f(x) = 44.4172x + 7.597699\\end{equation}\n",
    "\n",
    "We can use this for each *x* (Study) value to calculate the *y* values for the regression line (*f(x)*), and we can subtract the original *y* (Grade) from these to calculate the error for each point:\n",
    "\n",
    "| Name     | Study | Grade | *f(x)* | Error |\n",
    "|----------|-------|-------|------|------ |\n",
    "| Dan      | 1     | 50    |52.0149 |2.0149 |\n",
    "| Joann    | 0.75  | 50    |40.9106 |-9.0894|\n",
    "| Pedro    | 0.6   | 46    |34.2480 |-11.752|\n",
    "| Rosie    | 2     | 95    |96.4321 |1.4321 |\n",
    "| Ethan    | 1     | 50    |52.0149 |2.0149 |\n",
    "| Vicky    | 0.2   | 5     |16.4811 |11.4811|\n",
    "| Frederic | 1.2   | 57    |60.8983 |3.8983 |\n",
    "\n",
    "As you can see, the *f(x)* values are mostly quite close to the actual *Grade* values, and the errors (which when we're comparing estimated values from a function with actual known values we we often call *residuals*) are generally pretty small.\n",
    "\n",
    "Let's plot the least squares regression line with the actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic'],\n",
    "                   'Study':[1,0.75,0.6,2,1,0.2,1.2],\n",
    "                   'Grade':[50,50,46,95,50,5,57],\n",
    "                   'fx':[52.0159,40.9106,34.2480,96.4321,52.0149,16.4811,60.8983]})\n",
    "\n",
    "# Create a scatter plot of Study vs Grade\n",
    "df.plot(kind='scatter', title='Study Time vs Grade Regression', x='Study', y='Grade', color='red')\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(df['Study'],df['fx'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the line fits the middle values fairly well, but is less accurate for the outlier at the low end. This is often the case, which is why statisticians and data scientists often *treat* outliers by removing them or applying a threshold value; though in this example there are too few data points to conclude that the data points are really outliers.\n",
    "\n",
    "Let's look at a slightly larger dataset and apply the same approach to compare *Grade* and *Salary*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n",
    "                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n",
    "                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n",
    "\n",
    "# Calculate least squares regression line\n",
    "df['x2'] = df['Grade']**2\n",
    "df['xy'] = df['Grade'] * df['Salary']\n",
    "x = df['Grade'].sum()\n",
    "y = df['Salary'].sum()\n",
    "x2 = df['x2'].sum()\n",
    "xy = df['xy'].sum()\n",
    "n = df['Grade'].count()\n",
    "m = ((n*xy) - (x*y))/((n*x2)-(x**2))\n",
    "b = (y - (m*x))/n\n",
    "df['fx'] = (m*df['Grade']) + b\n",
    "df['error'] = df['fx'] - df['Salary']\n",
    "\n",
    "print('slope: ' + str(m))\n",
    "print('y-intercept: ' + str(b))\n",
    "\n",
    "# Create a scatter plot of Grade vs Salary\n",
    "df.plot(kind='scatter', title='Grade vs Salary Regression', x='Grade', y='Salary', color='red')\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(df['Grade'],df['fx'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Show the original x,y values, the f(x) value, and the error\n",
    "df[['Grade', 'Salary', 'fx', 'error']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we used Python expressions to calculate the *slope* and *y-intercept* using the same approach and formula as before. In practice, Python provides great support for statistical operations like this; and you can use the ***linregress*** function in the *scipy.stats* package to retrieve the *slope* and *y-intercept* (as well as the *correlation*, *p-value*, and *standard error*) for a matched array of *x* and *y* values (we'll discuss *p-values* later!).\n",
    "\n",
    "Here's the Python code to calculate the regression line variables using the ***linregress*** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n",
    "                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n",
    "                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n",
    "\n",
    "# Get the regression line slope and intercept\n",
    "m, b, r, p, se = stats.linregress(df['Grade'], df['Salary'])\n",
    "\n",
    "df['fx'] = (m*df['Grade']) + b\n",
    "df['error'] = df['fx'] - df['Salary']\n",
    "\n",
    "print('slope: ' + str(m))\n",
    "print('y-intercept: ' + str(b))\n",
    "\n",
    "# Create a scatter plot of Grade vs Salary\n",
    "df.plot(kind='scatter', title='Grade vs Salary Regression', x='Grade', y='Salary', color='red')\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(df['Grade'],df['fx'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Show the original x,y values, the f(x) value, and the error\n",
    "df[['Grade', 'Salary', 'fx', 'error']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the *slope* and *y-intercept* values are the same as when we worked them out using the formula.\n",
    "\n",
    "Similarly to the simple study hours example, the regression line doesn't fit the outliers very well. In this case, the extremes include a student who scored only 5, and a student who scored 95. Let's see what happens if we remove these students from our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', 'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny'],\n",
    "                   'Salary':[50000,54000,50000,189000,55000,40000,59000,42000,47000,78000,119000,95000,49000,29000,130000],\n",
    "                   'Hours':[41,40,36,17,35,39,40,45,41,35,30,33,38,47,24],\n",
    "                   'Grade':[50,50,46,95,50,5,57,42,26,72,78,60,40,17,85]})\n",
    "\n",
    "df = df[(df['Grade'] > 5) & (df['Grade'] < 95)]\n",
    "\n",
    "# Get the regression line slope and intercept\n",
    "m, b, r, p, se = stats.linregress(df['Grade'], df['Salary'])\n",
    "\n",
    "df['fx'] = (m*df['Grade']) + b\n",
    "df['error'] = df['fx'] - df['Salary']\n",
    "\n",
    "print('slope: ' + str(m))\n",
    "print('y-intercept: ' + str(b))\n",
    "\n",
    "# Create a scatter plot of Grade vs Salary\n",
    "df.plot(kind='scatter', title='Grade vs Salary Regression', x='Grade', y='Salary', color='red')\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(df['Grade'],df['fx'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Show the original x,y values, the f(x) value, and the error\n",
    "df[['Grade', 'Salary', 'fx', 'error']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the outliers removed, the line is a slightly better overall fit to the data.\n",
    "\n",
    "One of the neat things about regression is that it gives us a formula and some constant values that we can use to estimate a *y* value for any *x* value. We just need to apply the linear function using the *slope* and *y-intercept* values we've calculated from our sample data. For example, suppose a student named Fabio graduates from our school with a final grade of **62**. We can use our linear function with the *slope* and *y-intercept* values we calculated with Python to estimate what salary he can expect to earn:\n",
    "\n",
    "\\begin{equation}f(x) = (1424.50\\times62) - 7822.24 \\approx 80,497 \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "Many of the problems we try to solve using statistics are to do with *probability*. For example, what's the probable salary for a graduate who scored a given score in their final exam at school? Or, what's the likely height of a child given the height of his or her parents?\n",
    "\n",
    "It therefore makes sense to learn some basic principles of probability as we study statistics.\n",
    "\n",
    "## Probability Basics\n",
    "Let's start with some basic definitions and principles.\n",
    "- An ***experiment*** or ***trial*** is an action with an uncertain outcome, such as tossing a coin.\n",
    "- A ***sample space*** is the set of all possible outcomes of an experiment. In a coin toss, there's a set of two possible oucomes (*heads* and *tails*).\n",
    "- A ***sample point*** is a single possible outcome - for example, *heads*)\n",
    "- An ***event*** is a specific outome of single instance of an experiment - for example, tossing a coin and getting *tails*.\n",
    "- ***Probability*** is a value between 0 and 1 that indicates the likelihood of a particular event, with 0 meaning that the event is impossible, and 1 meaning that the event is inevitable. In general terms, it's calculated like this:\n",
    "\n",
    "\\begin{equation}\\text{probability of an event} = \\frac{\\text{Number of sample points that produce the event}}{\\text{Total number of sample points in the sample space}} \\end{equation}\n",
    "\n",
    "For example, the probability of getting *heads* when tossing as coin is <sup>1</sup>/<sub>2</sub> - there is only one side of the coin that is designated *heads*. and there are two possible outcomes in the sample space (*heads* and *tails*). So the probability of getting *heads* in a single coin toss is 0.5 (or 50% when expressed as a percentage).\n",
    "\n",
    "Let's look at another example. Suppose you throw two dice, hoping to get 7.\n",
    "\n",
    "The dice throw itself is an *experiment* - you don't know the outome until the dice have landed and settled.\n",
    "\n",
    "The *sample space* of all possible outcomes is every combination of two dice - 36 *sample points*:\n",
    "<table style='font-size:36px;'>\n",
    "<tr><td>&#9856;+&#9856;</td><td>&#9856;+&#9857;</td><td>&#9856;+&#9858;</td><td>&#9856;+&#9859;</td><td>&#9856;+&#9860;</td><td>&#9856;+&#9861;</td></tr>\n",
    "<tr><td>&#9857;+&#9856;</td><td>&#9857;+&#9857;</td><td>&#9857;+&#9858;</td><td>&#9857;+&#9859;</td><td>&#9857;+&#9860;</td><td>&#9857;+&#9861;</td></tr>\n",
    "<tr><td>&#9858;+&#9856;</td><td>&#9858;+&#9857;</td><td>&#9858;+&#9858;</td><td>&#9858;+&#9859;</td><td>&#9858;+&#9860;</td><td>&#9858;+&#9861;</td></tr>\n",
    "<tr><td>&#9859;+&#9856;</td><td>&#9859;+&#9857;</td><td>&#9859;+&#9858;</td><td>&#9859;+&#9859;</td><td>&#9859;+&#9860;</td><td>&#9859;+&#9861;</td></tr>\n",
    "<tr><td>&#9860;+&#9856;</td><td>&#9860;+&#9857;</td><td>&#9860;+&#9858;</td><td>&#9860;+&#9859;</td><td>&#9860;+&#9860;</td><td>&#9860;+&#9861;</td></tr>\n",
    "<tr><td>&#9861;+&#9856;</td><td>&#9861;+&#9857;</td><td>&#9861;+&#9858;</td><td>&#9861;+&#9859;</td><td>&#9861;+&#9860;</td><td>&#9861;+&#9861;</td></tr>\n",
    "</table>\n",
    "\n",
    "The *event* you want to happen is throwing a 7. There are 6 *sample points* that could produce this event:\n",
    "\n",
    "<table style='font-size:36px;'>\n",
    "<tr><td style='color:lightgrey;'>&#9856;+&#9856;</td><td style='color:lightgrey;'>&#9856;+&#9857;</td><td style='color:lightgrey;'>&#9856;+&#9858;</td><td style='color:lightgrey;'>&#9856;+&#9859;</td><td style='color:lightgrey;'>&#9856;+&#9860;</td><td>&#9856;+&#9861;</td></tr>\n",
    "<tr><td style='color:lightgrey;'>&#9857;+&#9856;</td><td style='color:lightgrey;'>&#9857;+&#9857;</td><td style='color:lightgrey;'>&#9857;+&#9858;</td><td style='color:lightgrey;'>&#9857;+&#9859;</td><td>&#9857;+&#9860;</td><td style='color:lightgrey;'>&#9857;+&#9861;</td></tr>\n",
    "<tr><td style='color:lightgrey;'>&#9858;+&#9856;</td><td style='color:lightgrey;'>&#9858;+&#9857;</td><td style='color:lightgrey;'>&#9858;+&#9858;</td><td>&#9858;+&#9859;</td><td style='color:lightgrey;'>&#9858;+&#9860;</td><td style='color:lightgrey;'>&#9858;+&#9861;</td></tr>\n",
    "<tr><td style='color:lightgrey;'>&#9859;+&#9856;</td><td style='color:lightgrey;'>&#9859;+&#9857;</td><td>&#9859;+&#9858;</td><td style='color:lightgrey;'>&#9859;+&#9859;</td><td style='color:lightgrey;'>&#9859;+&#9860;</td><td style='color:lightgrey;'>&#9859;+&#9861;</td></tr>\n",
    "<tr><td style='color:lightgrey;'>&#9860;+&#9856;</td><td>&#9860;+&#9857;</td><td style='color:lightgrey;'>&#9860;+&#9858;</td><td style='color:lightgrey;'>&#9860;+&#9859;</td><td style='color:lightgrey;'>&#9860;+&#9860;</td><td style='color:lightgrey;'>&#9860;+&#9861;</td></tr>\n",
    "<tr><td>&#9861;+&#9856;</td><td style='color:lightgrey;'>&#9861;+&#9857;</td><td style='color:lightgrey;'>&#9861;+&#9858;</td><td style='color:lightgrey;'>&#9861;+&#9859;</td><td style='color:lightgrey;'>&#9861;+&#9860;</td><td style='color:lightgrey;'>&#9861;+&#9861;</td></tr>\n",
    "</table>\n",
    "\n",
    "The *probability* of throwing a 7 is therefore <sup>6</sup>/<sub>36</sub> which can be simplified to <sup>1</sup>/<sub>6</sub> or approximately 0.167 (16.7%).\n",
    "\n",
    "### Probability Notation\n",
    "When we express probability, we use an upper-case **P** to indicate *probability* and an upper-case letter to represent the event. So to express the probability of throwing a 7 as an event valled ***A***, we could write:\n",
    "\n",
    "\\begin{equation}P(A) = 0.167 \\end{equation}\n",
    "\n",
    "### The Complement of an Event\n",
    "The *complement* of an event is the set of *sample points* that do ***not*** result in the event.\n",
    "\n",
    "For example, suppose you have a standard deck of playing cards, and you draw one card, hoping for a *spade*. In this case, the drawing of a card is the *experiment*, and the *event* is drawing a spade. There are 13 cards of each suit in the deck. So the *sample space* contains 52 *sample points*:\n",
    "\n",
    "<table>\n",
    "<tr><td>13 x <span style='font-size:32px;color:red;'>&hearts;</span></td><td>13 x <span style='font-size:32px;color:black;'>&spades;</span></td><td>13 x <span style='font-size:32px;color:black;'>&clubs;</span></td><td>13 x <span style='font-size:32px;color:red;'>&diams;</span></td></tr>\n",
    "</table>\n",
    "\n",
    "There are 13 *sample points* that would satisfy the requirements of the event:\n",
    "\n",
    "<table>\n",
    "<tr><td style='color:lightgrey;'>13 x <span style='font-size:32px;'>&hearts;</span></td><td>13 x <span style='font-size:32px;'>&spades;</span></td><td style='color:lightgrey;'>13 x <span style='font-size:32px;'>&clubs;</span></td><td style='color:lightgrey;'>13 x <span style='font-size:32px'>&diams;</span></td></tr>\n",
    "</table>\n",
    "\n",
    "So the *probability* of the event (drawing a spade) is <sup>13</sup>/<sub>52</sub> which is <sup>1</sup>/<sub>4</sub> or 0.25 (25%).\n",
    "\n",
    "The *complement* of the event is all of the possible outcomes that *don't* result in drawing a spade:\n",
    "\n",
    "<table>\n",
    "<tr><td>13 x <span style='font-size:32px;color:red;'>&hearts;</span></td><td style='color:lightgrey;'>13 x <span style='font-size:32px;'>&spades;</span></td><td>13 x <span style='font-size:32px;color:black;'>&clubs;</span></td><td>13 x <span style='font-size:32px;color:red;'>&diams;</span></td></tr>\n",
    "</table>\n",
    "\n",
    "There are 39 sample points in the complement (3 x 13), so the probability of the complement is <sup>39</sup>/<sub>52</sub> which is <sup>3</sup>/<sub>4</sub> or 0.75 (75%).\n",
    "\n",
    "Note that the probability of an event and the probability of its complement ***always add up to 1***.\n",
    "\n",
    "This fact can be useful in some cases. For example, suppose you throw two dice and want to know the probability of throwing more than 4. You *could* count all of the outcomes that would produce this result, but there are a lot of them. It might be easier to identify the ones that *do not* produce this result (in other words, the complement):\n",
    "\n",
    "<table style='font-size:36px;'>\n",
    "<tr><td>&#9856;+&#9856;</td><td>&#9856;+&#9857;</td><td>&#9856;+&#9858;</td><td style='color:lightgrey;'>&#9856;+&#9859;</td><td style='color:lightgrey;'>&#9856;+&#9860;</td><td style='color:lightgrey;'>&#9856;+&#9861;</td></tr>\n",
    "<tr><td>&#9857;+&#9856;</td><td>&#9857;+&#9857;</td><td style='color:lightgrey;'>&#9857;+&#9858;</td><td style='color:lightgrey;'>&#9857;+&#9859;</td><td style='color:lightgrey;'>&#9857;+&#9860;</td><td style='color:lightgrey;'>&#9857;+&#9861;</td></tr>\n",
    "<tr><td>&#9858;+&#9856;</td><td style='color:lightgrey;'>&#9858;+&#9857;</td><td style='color:lightgrey;'>&#9858;+&#9858;</td><td style='color:lightgrey;'>&#9858;+&#9859;</td><td style='color:lightgrey;'>&#9858;+&#9860;</td><td style='color:lightgrey;'>&#9858;+&#9861;</td></tr>\n",
    "<tr><td style='color:lightgrey;'>&#9859;+&#9856;</td><td style='color:lightgrey;'>&#9859;+&#9857;</td><td style='color:lightgrey;'>&#9859;+&#9858;</td><td style='color:lightgrey;'>&#9859;+&#9859;</td><td style='color:lightgrey;'>&#9859;+&#9860;</td><td style='color:lightgrey;'>&#9859;+&#9861;</td></tr>\n",
    "<tr><td style='color:lightgrey;'>&#9860;+&#9856;</td><td style='color:lightgrey;'>&#9860;+&#9857;</td><td style='color:lightgrey;'>&#9860;+&#9858;</td><td style='color:lightgrey;'>&#9860;+&#9859;</td><td style='color:lightgrey;'>&#9860;+&#9860;</td><td style='color:lightgrey;'>&#9860;+&#9861;</td></tr>\n",
    "<tr><td style='color:lightgrey;'>&#9861;+&#9856;</td><td style='color:lightgrey;'>&#9861;+&#9857;</td><td style='color:lightgrey;'>&#9861;+&#9858;</td><td style='color:lightgrey;'>&#9861;+&#9859;</td><td style='color:lightgrey;'>&#9861;+&#9860;</td><td style='color:lightgrey;'>&#9861;+&#9861;</td></tr>\n",
    "</table>\n",
    "\n",
    "Out of a total of 36 sample points in the sample space, there are 6 sample points where you throw a 4 or less (1+1, 1+2, 1+3, 2+1, 2+2, and 3+1); so the probability of the complement is <sup>6</sup>/<sub>36</sub> which is <sup>1</sup>/<sub>6</sub> or approximately 0.167 (16.7%).\n",
    "\n",
    "Now, here's the clever bit. Since the probability of the complement and the event itself must add up to 1, the probability of the event must be **<sup>5</sup>/<sub>6</sub>** or **0.833** (**83.3%**).\n",
    "\n",
    "We indicate the complement of an event by adding a **'** to the letter assigned to it, so:\n",
    "\n",
    "\\begin{equation}P(A) = 1 - P(A') \\end{equation}\n",
    "\n",
    "### Bias\n",
    "Often, the sample points in the sample space do not have the same probability, so there is a *bias* that makes one outcome more likely than another. For example, suppose your local weather forecaster indicates the predominant weather for each day of the week like this:\n",
    "\n",
    "<table>\n",
    "<tr><td style='text-align:center'>Mon</td><td style='text-align:center'>Tue</td><td style='text-align:center'>Wed</td><td style='text-align:center'>Thu</td><td style='text-align:center'>Fri</td><td style='text-align:center'>Sat</td><td style='text-align:center'>Sun</td></tr>\n",
    "<tr style='font-size:32px'><td>&#9729;</td><td>&#9730;</td><td>&#9728;</td><td>&#9728;</td><td>&#9728;</td><td>&#9729;</td><td>&#9728;</td></tr>\n",
    "</table>\n",
    "\n",
    "This forceast is pretty typical for your area at this time of the year. In fact, historically the weather is sunny on 60% of days, cloudy on 30% of days, and rainy on only 10% of days. On any given day, the sample space for the weather contains 3 sample points (*sunny*, *cloudy*, and *rainy*); but the probabities for these sample points are not the same.\n",
    "\n",
    "If we assign the letter **A** to a sunny day event, **B** to a cloudy day event, and **C** to a rainy day event then we can write these probabilities like this:\n",
    "\n",
    "\\begin{equation}P(A)=0.6\\;\\;\\;\\; P(B)=0.3\\;\\;\\;\\; P(C)=0.1 \\end{equation}\n",
    "\n",
    "The complement of **A** (a sunny day) is any day where it is not sunny - it is either cloudy or rainy. We can work out the probability for this in two ways: we can subtract the probablity of **A** from 1:\n",
    "\n",
    "\\begin{equation}P(A') = 1 - P(A) = 1 - 0.6 = 0.4 \\end{equation}\n",
    "\n",
    "Or we can add together the probabilities for all events that do *not* result in a sunny day:\n",
    "\n",
    "\\begin{equation}P(A') = P(B) + P(C) = 0.3 + 0.1 = 0.4 \\end{equation}\n",
    "\n",
    "Either way, there's a 40% chance of it not being sunny!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability and Dependence\n",
    "Events can be:\n",
    "- *Independent* (events that are not affected by other events)\n",
    "- *Dependent* (events that are conditional on other events)\n",
    "- *Mutually Exclusive* (events that can't occur together)\n",
    "\n",
    "### Independent Events\n",
    "Imagine you toss a coin. The sample space contains two possible outomes: heads (<span style='font-size:42px;color:gold;'><sub>&#10050;</sub></span>) or tails (<span style='font-size:42px;color:gold;'><sub>&#9854;</sub></span>).\n",
    "\n",
    "The probability of getting *heads* is <sup>1</sup>/<sub>2</sub>, and the probability of getting *tails* is also <sup>1</sup>/<sub>2</sub>. Let's toss a coin...\n",
    "\n",
    "<span style='font-size:48px;color:gold;'>&#10050;</span>\n",
    "\n",
    "OK, so we got *heads*. Now, let's toss the coin again:\n",
    "\n",
    "<span style='font-size:48px;color:gold;'>&#10050;</span>\n",
    "\n",
    "It looks like we got *heads* again. If we were to toss the coin a third time, what's the probability that we'd get *heads*?\n",
    "\n",
    "Although you might be tempted to think that a *tail* is overdue, the fact is that each coin toss is an independent event. The outcome of the first coin toss does not affect the second coin toss (or the third, or any number of other coin tosses). For each independent coin toss, the probability of getting *heads* (or *tails*) remains <sup>1</sup>/<sub>2</sub>, or 50%.\n",
    "\n",
    "Run the following Python code to simulate 10,000 coin tosses by assigning a random value of 0 or 1 to *heads* and *tails*. Each time the coin is tossed, the probability of getting *heads* or *tails* is 50%, so you should expect approximately half of the results to be *heads* and half to be *tails* (it won't be exactly half, due to a little random variation; but it should be close):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "# Create a list with 2 element (for heads and tails)\n",
    "heads_tails = [0,0]\n",
    "\n",
    "# loop through 10000 trials\n",
    "trials = 10000\n",
    "trial = 0\n",
    "while trial < trials:\n",
    "    trial = trial + 1\n",
    "    # Get a random 0 or 1\n",
    "    toss = random.randint(0,1)\n",
    "    # Increment the list element corresponding to the toss result\n",
    "    heads_tails[toss] = heads_tails[toss] + 1\n",
    "\n",
    "print (heads_tails)\n",
    "\n",
    "# Show a pie chart of the results\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.pie(heads_tails, labels=['heads', 'tails'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Independent Events\n",
    "Now, let's ask a slightly different question. What is the probability of getting three *heads* in a row? Since the probability of a heads on each independent toss is <sup>1</sup>/<sub>2</sub>, you might be tempted to think that the same probability applies to getting three *heads* in a row; but actually, we need to treat getting three *heads* as it's own event, which is the combination of three independent events. To combine independent events like this, we need to multiply the probability of each event. So:\n",
    "\n",
    "<span style='font-size:48px;color:gold;'><sub>&#10050;</sub></span> = <sup>1</sup>/<sub>2</sub>\n",
    "\n",
    "<span style='font-size:48px;color:gold;'><sub>&#10050;&#10050;</sub></span> = <sup>1</sup>/<sub>2</sub> x <sup>1</sup>/<sub>2</sub>\n",
    "\n",
    "<span style='font-size:48px;color:gold;'><sub>&#10050;&#10050;&#10050;</sub></span> = <sup>1</sup>/<sub>2</sub> x <sup>1</sup>/<sub>2</sub> x <sup>1</sup>/<sub>2</sub>\n",
    "\n",
    "So the probability of tossing three *heads* in a row is 0.5 x 0.5 x 0.5, which is 0.125 (or 12.5%).\n",
    "\n",
    "Run the code below to simulate 10,000 trials of flipping a coin three times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Count the number of 3xHeads results\n",
    "h3 = 0\n",
    "\n",
    "# Create a list of all results\n",
    "results = []\n",
    "\n",
    "# loop through 10000 trials\n",
    "trials = 10000\n",
    "trial = 0\n",
    "while trial < trials:\n",
    "    trial = trial + 1\n",
    "    # Flip three coins\n",
    "    result = ['H' if random.randint(0,1) == 1 else 'T',\n",
    "              'H' if random.randint(0,1) == 1 else 'T',\n",
    "              'H' if random.randint(0,1) == 1 else 'T']\n",
    "    results.append(result)\n",
    "    # If it's three heads, add it to the count\n",
    "    h3 = h3 + int(result == ['H','H','H'])\n",
    "    \n",
    "# What proportion of trials produced 3x heads\n",
    "print (\"%.2f%%\" % ((h3/trials)*100))\n",
    "\n",
    "# Show all the results\n",
    "print (results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows the percentage of times a trial resulted in three heads (which should be somewhere close to 12.5%). You can count the number of *['H', 'H', 'H']* entries in the full list of results to verify this if you like!\n",
    "\n",
    "\n",
    "#### Probability Trees\n",
    "You can represent a series of events and their probabilities as a probability tree:\n",
    "\n",
    "                         ____H(0.5)  : 0.5 x 0.5 x 0.5 = 0.125\n",
    "                        /\n",
    "                   ____H(0.5)\n",
    "                  /     \\____T(0.5)  : 0.5 x 0.5 x 0.5 = 0.125\n",
    "                 /        \n",
    "              __H(0.5)   ____H(0.5)  : 0.5 x 0.5 x 0.5 = 0.125\n",
    "             /   \\      / \n",
    "            /     \\____T(0.5)\n",
    "           /            \\____T(0.5)  : 0.5 x 0.5 x 0.5 = 0.125\n",
    "          /              \n",
    "    _____/              _____H(0.5)  : 0.5 x 0.5 x 0.5 = 0.125\n",
    "         \\             / \n",
    "          \\        ___H(0.5)\n",
    "           \\      /    \\_____T(0.5)  : 0.5 x 0.5 x 0.5 = 0.125\n",
    "            \\    /       \n",
    "             \\__T(0.5)  _____H(0.5)  : 0.5 x 0.5 x 0.5 = 0.125\n",
    "                 \\     /\n",
    "                  \\___T(0.5)\n",
    "                       \\_____T(0.5)  : 0.5 x 0.5 x 0.5 = 0.125\n",
    "                                                         _____\n",
    "                                                          1.0\n",
    "                        \n",
    "Starting at the left, you can follow the branches in the tree that represent each event (in this case a coin toss result of *heads* or *tails* at each branch). Multiplying the probability of each branch of your path through the tree gives you the combined probability for an event composed of all of the events in the path. In this case, you can see from the tree that you are equally likely to get any sequence of three *heads* or *tails* results (so three *heads* is just as likely as three *tails*, which is just as likely as *head-tail-head*, *tail-head-tail*, or any other combination!)\n",
    "\n",
    "Note that the total probability for all paths through the tree adds up to 1.\n",
    "\n",
    "#### Combined Event Probability Notation\n",
    "When calculating the probability of combined events, we assign a letter such as **A** or **B** to each event, and we use the *intersection* (**&cap;**) symbol to indicate that we want the combined probability of multiple events. So we could assign the letters **A**, **B**, and **C** to each independent coin toss in our sequence of three tosses, and express the combined probability like this:\n",
    "\n",
    "\\begin{equation}P(A \\cap B \\cap C) = P(A) \\times P(B) \\times P(C) \\end{equation}\n",
    "\n",
    "#### Combining Events with Different Probabilities\n",
    "Imagine you have created a new game that mixes the excitment of coin-tossing with the thrill of die-rolling! The objective of the game is to roll a die and get *6*, and toss a coin and get *heads*:\n",
    "\n",
    "<div style='text-align:center'><span style='font-size:48px;'>&#9861;</span><span style='font-size:42px;'> +</span><span style='font-size:48px;color:gold;'>&#10050;</span></div>\n",
    "\n",
    "On each turn of the game, a player rolls the die and tosses the coin.\n",
    "\n",
    "How can we calculate the probability of winning?\n",
    "\n",
    "There are two independent events required to win: a die-roll of *6* (which we'll call event **A**), and a coin-toss of *heads* (which we'll call event **B**)\n",
    "\n",
    "Our formula for combined independent events is:\n",
    "\n",
    "\\begin{equation}P(A \\cap B) = P(A) \\times P(B) \\end{equation}\n",
    "\n",
    "The probablilty of rolling a *6* on a fair die is <sup>1</sup>/<sub>6</sub> or 0.167;  and the probability of tossing a coin and getting *heads* is <sup>1</sup>/<sub>2</sub> or 0.5:\n",
    "\n",
    "\\begin{equation}P(A \\cap B) = 0.167 \\times 0.5 = 0.083 \\end{equation}\n",
    "\n",
    "So on each turn, there's an 8.3% chance to win the game.\n",
    "\n",
    "#### Intersections and Unions\n",
    "\n",
    "Previously you saw that we use the *intersection* (**&cap;**) symbol to represent \"and\" when combining event probabilities. This notation comes from a branch of mathematics called *set theory*, in which we work with sets of values. let's examine this in a little more detail.\n",
    "\n",
    "Here's our deck of playing cards, with the full sample space for drawing any card:\n",
    "\n",
    "<table style='font-size:18px;'>\n",
    "<tr><td style='color:red;'>A &hearts;</td><td style='color:black;'>A &spades;</td><td style='color:black;'>A &clubs;<td style='color:red;'>A &diams;</td></tr>\n",
    "<tr><td style='color:red;'>K &hearts;</td><td style='color:black;'>K &spades;</td><td style='color:black;'>K &clubs;<td style='color:red;'>K &diams;</td></tr>\n",
    "<tr><td style='color:red;'>Q &hearts;</td><td style='color:black;'>Q &spades;</td><td style='color:black;'>Q &clubs;<td style='color:red;'>Q &diams;</td></tr>\n",
    "<tr><td style='color:red;'>J &hearts;</td><td style='color:black;'>J &spades;</td><td style='color:black;'>J &clubs;<td style='color:red;'>J &diams;</td></tr>\n",
    "<tr><td style='color:red;'>10 &hearts;</td><td style='color:black;'>10 &spades;</td><td style='color:black;'>10 &clubs;<td style='color:red;'>10 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>9 &hearts;</td><td style='color:black;'>9 &spades;</td><td style='color:black;'>9 &clubs;<td style='color:red;'>9 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>8 &hearts;</td><td style='color:black;'>8 &spades;</td><td style='color:black;'>8 &clubs;<td style='color:red;'>8 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>7 &hearts;</td><td style='color:black;'>7 &spades;</td><td style='color:black;'>7 &clubs;<td style='color:red;'>7 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>6 &hearts;</td><td style='color:black;'>6 &spades;</td><td style='color:black;'>6 &clubs;<td style='color:red;'>6 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>5 &hearts;</td><td style='color:black;'>5 &spades;</td><td style='color:black;'>5 &clubs;<td style='color:red;'>5 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>4 &hearts;</td><td style='color:black;'>4 &spades;</td><td style='color:black;'>4 &clubs;<td style='color:red;'>4 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>3 &hearts;</td><td style='color:black;'>3 &spades;</td><td style='color:black;'>3 &clubs;<td style='color:red;'>3 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>2 &hearts;</td><td style='color:black;'>2 &spades;</td><td style='color:black;'>2 &clubs;<td style='color:red;'>2 &diams;</td></tr>\n",
    "</table>\n",
    "\n",
    "Now, let's look at two potential events:\n",
    "- Drawing an ace (**A**)\n",
    "- Drawing a red card (**B**)\n",
    "\n",
    "The set of sample points for event **A** (drawing an ace) is:\n",
    "\n",
    "<table style='font-size:18px;'>\n",
    "<tr><td style='color:red;'>A &hearts;</td><td style='color:black;'>A &spades;</td><td style='color:black;'>A &clubs;<td style='color:red;'>A &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>K &hearts;</td><td style='color:lightgrey;'>K &spades;</td><td style='color:lightgrey;'>K &clubs;<td>K &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>Q &hearts;</td><td>Q &spades;</td><td>Q &clubs;<td>Q &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>J &hearts;</td><td>J &spades;</td><td>J &clubs;<td>J &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>10 &hearts;</td><td>10 &spades;</td><td>10 &clubs;<td>10 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>9 &hearts;</td><td>9 &spades;</td><td>9 &clubs;<td>9 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>8 &hearts;</td><td>8 &spades;</td><td>8 &clubs;<td>8 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>7 &hearts;</td><td>7 &spades;</td><td>7 &clubs;<td>7 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>6 &hearts;</td><td>6 &spades;</td><td>6 &clubs;<td>6 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>5 &hearts;</td><td>5 &spades;</td><td>5 &clubs;<td>5 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>4 &hearts;</td><td>4 &spades;</td><td>4 &clubs;<td>4 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>3 &hearts;</td><td>3 &spades;</td><td>3 &clubs;<td>3 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>2 &hearts;</td><td>2 &spades;</td><td>2 &clubs;<td>2 &diams;</td></tr>\n",
    "</table>\n",
    "\n",
    "So the probability of drawing an ace is:\n",
    "\n",
    "\\begin{equation}P(A) = \\frac{4}{52} = \\frac{1}{13} = 0.077\\end{equation} \n",
    "\n",
    "Now let's look at the set of sample points for event **B** (drawing a red card)\n",
    "\n",
    "<table style='font-size:18px;'>\n",
    "<tr><td style='color:red;'>A &hearts;</td><td style='color:lightgrey;'>A &spades;</td><td style='color:lightgrey;'>A &clubs;<td style='color:red;'>A &diams;</td></tr>\n",
    "<tr><td style='color:red;'>K &hearts;</td><td style='color:lightgrey;'>K &spades;</td><td style='color:lightgrey;'>K &clubs;<td style='color:red;'>K &diams;</td></tr>\n",
    "<tr><td style='color:red;'>Q &hearts;</td><td style='color:lightgrey;'>Q &spades;</td><td style='color:lightgrey;'>Q &clubs;<td style='color:red;'>Q &diams;</td></tr>\n",
    "<tr><td style='color:red;'>J &hearts;</td><td style='color:lightgrey;'>J &spades;</td><td style='color:lightgrey;'>J &clubs;<td style='color:red;'>J &diams;</td></tr>\n",
    "<tr><td style='color:red;'>10 &hearts;</td><td style='color:lightgrey;'>10 &spades;</td><td style='color:lightgrey;'>10 &clubs;<td style='color:red;'>10 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>9 &hearts;</td><td style='color:lightgrey;'>9 &spades;</td><td style='color:lightgrey;'>9 &clubs;<td style='color:red;'>9 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>8 &hearts;</td><td style='color:lightgrey;'>8 &spades;</td><td style='color:lightgrey;'>8 &clubs;<td style='color:red;'>8 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>7 &hearts;</td><td style='color:lightgrey;'>7 &spades;</td><td style='color:lightgrey;'>7 &clubs;<td style='color:red;'>7 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>6 &hearts;</td><td style='color:lightgrey;'>6 &spades;</td><td style='color:lightgrey;'>6 &clubs;<td style='color:red;'>6 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>5 &hearts;</td><td style='color:lightgrey;'>5 &spades;</td><td style='color:lightgrey;'>5 &clubs;<td style='color:red;'>5 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>4 &hearts;</td><td style='color:lightgrey;'>4 &spades;</td><td style='color:lightgrey;'>4 &clubs;<td style='color:red;'>4 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>3 &hearts;</td><td style='color:lightgrey;'>3 &spades;</td><td style='color:lightgrey;'>3 &clubs;<td style='color:red;'>3 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>2 &hearts;</td><td style='color:lightgrey;'>2 &spades;</td><td style='color:lightgrey;'>2 &clubs;<td style='color:red;'>2 &diams;</td></tr>\n",
    "</table>\n",
    "\n",
    "The probability of drawing a red card is therefore:\n",
    "\n",
    "\\begin{equation}P(A) = \\frac{26}{52} = \\frac{1}{2} = 0.5\\end{equation} \n",
    "\n",
    "##### Intersections\n",
    "\n",
    "We can think of the sample spaces for these events as two sets, and we can show them as a Venn diagram:\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style='text-align:center'>Event A<span style='font-size:120px'>&#9901;</span>Event B</div>\n",
    "\n",
    "Each circle in the Venn diagram represents a set of sample points. The set on the left contains the sample points for event **A** (drawing an ace) and the set on the right contains the sample points for event **B** (drawing a red card). Note that the circles overlap, creating an intersection that contains only the sample points that apply to event **A** *and* event **B**.\n",
    "\n",
    "This intersected sample space looks like this:\n",
    "\n",
    "<table style='font-size:18px;'>\n",
    "<tr><td style='color:red;'>A &hearts;</td><td style='color:lightgrey;'>A &spades;</td><td style='color:lightgrey;'>A &clubs;<td style='color:red;'>A &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>K &hearts;</td><td style='color:lightgrey;'>K &spades;</td><td style='color:lightgrey;'>K &clubs;<td>K &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>Q &hearts;</td><td>Q &spades;</td><td>Q &clubs;<td>Q &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>J &hearts;</td><td>J &spades;</td><td>J &clubs;<td>J &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>10 &hearts;</td><td>10 &spades;</td><td>10 &clubs;<td>10 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>9 &hearts;</td><td>9 &spades;</td><td>9 &clubs;<td>9 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>8 &hearts;</td><td>8 &spades;</td><td>8 &clubs;<td>8 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>7 &hearts;</td><td>7 &spades;</td><td>7 &clubs;<td>7 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>6 &hearts;</td><td>6 &spades;</td><td>6 &clubs;<td>6 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>5 &hearts;</td><td>5 &spades;</td><td>5 &clubs;<td>5 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>4 &hearts;</td><td>4 &spades;</td><td>4 &clubs;<td>4 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>3 &hearts;</td><td>3 &spades;</td><td>3 &clubs;<td>3 &diams;</td></tr>\n",
    "<tr style='color:lightgrey;'><td>2 &hearts;</td><td>2 &spades;</td><td>2 &clubs;<td>2 &diams;</td></tr>\n",
    "</table>\n",
    "\n",
    "As you've seen previously, we write this as **A &cap; B**, and we can calculate its probability like this:\n",
    "\n",
    "\\begin{equation}P(A \\cap B) = P(A) \\times P(B) = 0.077 \\times 0.5 = 0.0385 \\end{equation}\n",
    "\n",
    "So when you draw a single card from a full deck, there is a 3.85% chance it will be a red ace.\n",
    "\n",
    "##### Unions\n",
    "The intersection describes the sample space for  event **A** *and* event **B**; but what if we wanted to look at the probability of drawing an ace *or* a red card. In other words, any sample point that is in either of the Venn digram circles.\n",
    "\n",
    "This set of sample points looks like this:\n",
    "\n",
    "<table style='font-size:18px;'>\n",
    "<tr><td style='color:red;'>A &hearts;</td><td style='color:black;'>A &spades;</td><td style='color:black;'>A &clubs;<td style='color:red;'>A &diams;</td></tr>\n",
    "<tr><td style='color:red;'>K &hearts;</td><td style='color:lightgrey;'>K &spades;</td><td style='color:lightgrey;'>K &clubs;<td style='color:red;'>K &diams;</td></tr>\n",
    "<tr><td style='color:red;'>Q &hearts;</td><td style='color:lightgrey;'>Q &spades;</td><td style='color:lightgrey;'>Q &clubs;<td style='color:red;'>Q &diams;</td></tr>\n",
    "<tr><td style='color:red;'>J &hearts;</td><td style='color:lightgrey;'>J &spades;</td><td style='color:lightgrey;'>J &clubs;<td style='color:red;'>J &diams;</td></tr>\n",
    "<tr><td style='color:red;'>10 &hearts;</td><td style='color:lightgrey;'>10 &spades;</td><td style='color:lightgrey;'>10 &clubs;<td style='color:red;'>10 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>9 &hearts;</td><td style='color:lightgrey;'>9 &spades;</td><td style='color:lightgrey;'>9 &clubs;<td style='color:red;'>9 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>8 &hearts;</td><td style='color:lightgrey;'>8 &spades;</td><td style='color:lightgrey;'>8 &clubs;<td style='color:red;'>8 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>7 &hearts;</td><td style='color:lightgrey;'>7 &spades;</td><td style='color:lightgrey;'>7 &clubs;<td style='color:red;'>7 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>6 &hearts;</td><td style='color:lightgrey;'>6 &spades;</td><td style='color:lightgrey;'>6 &clubs;<td style='color:red;'>6 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>5 &hearts;</td><td style='color:lightgrey;'>5 &spades;</td><td style='color:lightgrey;'>5 &clubs;<td style='color:red;'>5 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>4 &hearts;</td><td style='color:lightgrey;'>4 &spades;</td><td style='color:lightgrey;'>4 &clubs;<td style='color:red;'>4 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>3 &hearts;</td><td style='color:lightgrey;'>3 &spades;</td><td style='color:lightgrey;'>3 &clubs;<td style='color:red;'>3 &diams;</td></tr>\n",
    "<tr><td style='color:red;'>2 &hearts;</td><td style='color:lightgrey;'>2 &spades;</td><td style='color:lightgrey;'>2 &clubs;<td style='color:red;'>2 &diams;</td></tr>\n",
    "</table>\n",
    "\n",
    "We call this the *union* of the sets, and we write it as **A &cup; B**.\n",
    "\n",
    "To calculate the probability of a card being either an ace (of any color) or a red card (of any value), we can work out the probability of A, add it to the probability of B, and subtract the probability of A &cap; B (to avoid double-counting the red aces):\n",
    "\n",
    "\\begin{equation}P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\end{equation}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{equation}P(A \\cup B) = 0.077 + 0.5 - 0.0385 = 0.5385\\end{equation}\n",
    "\n",
    "So when you draw a card from a full deck, there is a 53.85% probability that it will be either an ace or a red card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent Events\n",
    "Let's return to our deck of 52 cards from which we're going to draw one card. The sample space can be summarized like this:\n",
    "\n",
    "<table>\n",
    "<tr><td>13 x <span style='font-size:32px;color:red;'>&hearts;</span></td><td>13 x <span style='font-size:32px;color:black;'>&spades;</span></td><td>13 x <span style='font-size:32px;color:black;'>&clubs;</span></td><td>13 x <span style='font-size:32px;color:red;'>&diams;</span></td></tr>\n",
    "</table>\n",
    "\n",
    "There are two black suits (*spades* and *clubs*) and two red suits (*hearts* and *diamonds*); with 13 cards in each suit. So the probability of drawing a black card (event **A**) and the probability of drawing a red card (event **B**) can be calculated like this:\n",
    "\n",
    "\\begin{equation}P(A) = \\frac{13 + 13}{52} = \\frac{26}{52} = 0.5 \\;\\;\\;\\; P(B) = \\frac{13 + 13}{52} = \\frac{26}{52} = 0.5\\end{equation}\n",
    "\n",
    "Now let's draw a card from the deck:\n",
    "\n",
    "<div style ='text-align:center;'><span style='font-size:32px;color:red;'>&hearts;</span></div>\n",
    "\n",
    "We drew a heart, which is red. So, assuming we don't replace the card back into the deck, this changes the sample space as follows:\n",
    "\n",
    "<table>\n",
    "<tr><td>12 x <span style='font-size:32px;color:red;'>&hearts;</span></td><td>13 x <span style='font-size:32px;color:black;'>&spades;</span></td><td>13 x <span style='font-size:32px;color:black;'>&clubs;</span></td><td>13 x <span style='font-size:32px;color:red;'>&diams;</span></td></tr>\n",
    "</table>\n",
    "\n",
    "The probabilities for **A** and **B** are now:\n",
    "\n",
    "\\begin{equation}P(A) = \\frac{13 + 13}{51} = \\frac{26}{51} = 0.51 \\;\\;\\;\\; P(B) = \\frac{12 + 13}{51} = \\frac{25}{51} = 0.49\\end{equation}\n",
    "\n",
    "Now let's draw a second card:\n",
    "\n",
    "<div style ='text-align:center;'><span style='font-size:32px;color:red;'>&diams;</span></div>\n",
    "\n",
    "We drew a diamond, so again this changes the sample space for the next draw:\n",
    "\n",
    "<table>\n",
    "<tr><td>12 x <span style='font-size:32px;color:red;'>&hearts;</span></td><td>13 x <span style='font-size:32px;color:black;'>&spades;</span></td><td>13 x <span style='font-size:32px;color:black;'>&clubs;</span></td><td>12 x <span style='font-size:32px;color:red;'>&diams;</span></td></tr>\n",
    "</table>\n",
    "\n",
    "The probabilities for **A** and **B** are now:\n",
    "\n",
    "\\begin{equation}P(A) = \\frac{13 + 13}{50} = \\frac{26}{50} = 0.52 \\;\\;\\;\\; P(B) = \\frac{12 + 12}{50} = \\frac{24}{50} = 0.48\\end{equation}\n",
    "\n",
    "So it's clear that one event can affect another; in this case, the probability of drawing a card of a particular color on the second draw depends on the color of card drawn on the previous draw. We call these *dependent* events.\n",
    "\n",
    "Probability trees are particularly useful when looking at dependent events. Here's a probability tree for drawing red or black cards as the first three draws from a deck of cards:\n",
    "\n",
    "                         _______R(0.48) \n",
    "                        /\n",
    "                   ____R(0.49)\n",
    "                  /     \\_______B(0.52) \n",
    "                 /        \n",
    "              __R(0.50)  _______R(0.50) \n",
    "             /   \\      / \n",
    "            /     \\____B(0.51)\n",
    "           /            \\_______B(0.50) \n",
    "          /              \n",
    "    _____/              ________R(0.50) \n",
    "         \\             / \n",
    "          \\        ___R(0.51)\n",
    "           \\      /    \\________B(0.50) \n",
    "            \\    /       \n",
    "             \\__B(0.50) ________R(0.52) \n",
    "                 \\     /\n",
    "                  \\___B(0.49)\n",
    "                       \\________B(0.48) \n",
    "\n",
    "\n",
    "\n",
    "#### Calculating Probabilities for Dependent Events\n",
    "Imagine a game in which you have to predict the color of the next card to be drawn. Suppose the first card drawn is a *spade*, which is black. What is the probability of the next card being red?\n",
    "\n",
    "The notation for this is:\n",
    "\n",
    "\\begin{equation}P(B|A)\\end{equation}\n",
    "\n",
    "You can interpret this as *the probability of B, given A*. In other words, given that event **A** (drawing a black card) has already happened, what is the probability of **B** (drawing a red card). This is commonly referred to as the *conditional probability* of B given A; and it's formula is:\n",
    "\n",
    "\\begin{equation}P(B|A) = \\frac{P(A \\cap B)}{P(A)}\\end{equation}\n",
    "\n",
    "So to return to our example, the probability of the second card being red given that the first card was black is:\n",
    "\n",
    "\\begin{equation}P(B|A) = \\frac{\\frac{26}{52} \\times \\frac{26}{51}}{\\frac{26}{52}}\\end{equation}\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "\\begin{equation}P(B|A) = \\frac{0.5 \\times 0.51}{0.5}\\end{equation}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{equation}P(B|A) = \\frac{0.255}{0.5} = 0.51\\end{equation}\n",
    "\n",
    "Which is what we calculated previously - so the formula works!\n",
    "\n",
    "Because this is an algebraic expression, we can rearrange it like this:\n",
    "\n",
    "\\begin{equation}P(A \\cap B) = P(A) \\times P(B|A)\\end{equation}\n",
    "\n",
    "We can use this form of the formula to calculate the probability that the first two cards drawn from a full deck of cards will both be jacks. In this case, event **A** is drawing a jack for the first card, and event **B** is drawing a jack for the second card.\n",
    "\n",
    "The probability that the first drawn card will be a jack is:\n",
    "\n",
    "\\begin{equation}P(A) = \\frac{4}{52} = \\frac{1}{13}\\end{equation}\n",
    "\n",
    "We draw the first card:\n",
    "\n",
    "<br/>\n",
    "<div style ='text-align:center;'><span style='font-size:32px;color:black;'>J &clubs;</span></div>\n",
    "\n",
    "Success! it's the jack of clubs. Our chances of the first two cards being jacks are looking good so far\n",
    "\n",
    "Now. we know that there are now only 3 jacks left, in a deck of 51 remaining cards; so the probability of drawing a jack as a second card, given that we drew a jack as the first card is:\n",
    "\n",
    "\\begin{equation}P(B|A) = \\frac{3}{51}\\end{equation}\n",
    "\n",
    "So we can work out the probability of drawing two jacks from a deck like this:\n",
    "\n",
    "\\begin{equation}P(A \\cap B) = \\frac{1}{13} \\times \\frac{3}{51} = \\frac{3}{663} = \\frac{1}{221}\\end{equation}\n",
    "\n",
    "So there's a 1 in 221 (0.45%)  probability that the first two cards drawn from a full deck will be jacks.\n",
    "\n",
    "\n",
    "### Mutually Exclusive Events\n",
    "We've talked about dependent and independent events, but there's a third category to be considered: mutually exclusive events.\n",
    "\n",
    "For example, when flipping a coin, what is the probability that in a single coin flip the result will be *heads* ***and*** *tails*? The answer is of course, 0; a single coin flip can only result in *heads* ***or*** *tails*; not both!\n",
    "\n",
    "For mutually exclusive event, the probability of an intersection is:\n",
    "\n",
    "\\begin{equation}P(A \\cap B) = 0\\end{equation}\n",
    "\n",
    "The probability for a union is:\n",
    "\n",
    "\\begin{equation}P(A \\cup B) = P(A) + P(B)\\end{equation}\n",
    "\n",
    "Note that we don't need to subtract the intersection (*and*) probability to calculate the union (*or*) probability like we did previously, because there's no risk of double-counting the sample points that lie in both events - there are none. (The intersection probability for mutually exclusive events is always 0, so you can subtract it if you like - you'll still get the same result!)\n",
    "\n",
    "Let's look at another two mutually exclusive events based on rolling a die:\n",
    "- Rolling a 6 (event **A**)\n",
    "- Rolling an odd number (event **B**)\n",
    "\n",
    "The probabilities for these events are:\n",
    "\n",
    "\\begin{equation}P(A) = \\frac{1}{6} \\;\\;\\;\\; P(B) = \\frac{3}{6}\\end{equation}\n",
    "\n",
    "What's the probability of rolling a 6 *and* an odd number in a single roll? These are mutually exclusive, so:\n",
    "\n",
    "\\begin{equation}P(A \\cap B) = 0\\end{equation}\n",
    "\n",
    "What's the probability of rolling a 6 *or* an odd number:\n",
    "\n",
    "\\begin{equation}P(A \\cup B) = \\frac{1}{6} + \\frac{3}{6} = \\frac{4}{6}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial Variables and Distributions\n",
    "Now that we know something about probability, let's apply that to statistics. Statistics is about inferring measures for a full population based on samples, allowing for random variation; so we're going to have to consider the idea of a *random variable*.\n",
    "\n",
    "A random variable us a number that can vary in value. For example, the temperature on a given day, or the number of students taking a class.\n",
    "\n",
    "### Binomial Variables\n",
    "One particular type of random variable that we use in statistics is a *binomial* variable. A binomial variable is used to count how frequently an event occurs in a fixed number of repeated independent experiments. The event in question must have the same probability of occurring in each experiment, and indicates the success or failure of the experiment; with a probability ***p*** of success, which has a complement of ***1 - p*** as the probability of failure (we often call this kind of experiment a *Bernoulli Trial* after Swiss mathematician Jacob Bernoulli).\n",
    "\n",
    "For example, suppose we flip a coin three times, counting *heads* as success. We can define a binomial variable to represent the number of successful coin flips (that is, the number of times we got *heads*).\n",
    "\n",
    "Let's examine this in more detail.\n",
    "\n",
    "We'll call our variable ***X***, and as stated previously it represents the number of times we flip *heads* in a series of three coin flips. Let's start by examining all the possible values for ***X***.\n",
    "\n",
    "We're flipping the coin three times, with a probability of <sup>1</sup>/<sub>2</sub> of success on each flip. The possibile results include none of the flips resulting in *heads*, all of the flips resulting in *heads*, or any combination in between. There are two possible outcomes from each flip, and there are three flips, so the total number of possible result sets is 2<sup>3</sup>, which is 8. Here they are:\n",
    "\n",
    "<div style='font-size:48px;color:gold;'>&#9854;&#9854;&#9854;</div>\n",
    "<br/>\n",
    "<div style='font-size:48px;color:gold;'>&#9854;&#10050;&#9854;</div>\n",
    "<br/>\n",
    "<div style='font-size:48px;color:gold;'>&#9854;&#9854;&#10050;</div>\n",
    "<br/>\n",
    "<div style='font-size:48px;color:gold;'>&#9854;&#10050;&#10050;</div>\n",
    "<br/>\n",
    "<div style='font-size:48px;color:gold;'>&#10050;&#9854;&#9854;</div>\n",
    "<br/>\n",
    "<div style='font-size:48px;color:gold;'>&#10050;&#10050;&#9854;</div>\n",
    "<br/>\n",
    "<div style='font-size:48px;color:gold;'>&#10050;&#9854;&#10050;</div>\n",
    "<br/>\n",
    "<div style='font-size:48px;color:gold;'>&#10050;&#10050;&#10050;</div>\n",
    "<br/>\n",
    "\n",
    "In these results, our variable ***X***, representing the number of successful events (getting *heads*), can vary from 0 to 3. We can write that like this:\n",
    "\n",
    "\\begin{equation}X=\\{0,1,2,3\\}\\end{equation}\n",
    "\n",
    "When we want to indicate a specific outcome for a random variable, we use write the variable in lower case, for example ***x*** So what's the probability that ***x*** = 0 (meaning that out of our three flips we got no *heads*)?\n",
    "\n",
    "We can easily see, that there is 1 row in our set of possible outcomes that contains no *heads*, so:\n",
    "\n",
    "\\begin{equation}P(x=0) = \\frac{1}{8}\\end{equation}\n",
    "\n",
    "OK, let's see if we can find the probability for 1 success. There are three sample points containing a single *heads* result, so:\n",
    "\n",
    "\\begin{equation}P(x=1) = \\frac{3}{8}\\end{equation}\n",
    "\n",
    "Again, we can easily see that from our results; but it's worth thinking about this in a slightly different way that will make it easier to calculate this probability more generically when there are more sample points (for example, if we had based our binomial variable on 100 coin flips, there would be many more combinations!).\n",
    "\n",
    "What we're actually saying here is that for **3** experiments (in this case coin flips), we want to *choose* **1** successful results. This is written as <sub>3</sub>C<sub>1</sub>. More generically, this is known as *n choose k*, and it's written like this:\n",
    "\n",
    "\\begin{equation}_{n}C_{k}\\end{equation}\n",
    "\n",
    "or sometimes like this:\n",
    "\n",
    "\\begin{equation}\\begin{pmatrix} n \\\\ k\\end{pmatrix}\\end{equation}\n",
    "\n",
    "The formula to calculate this is:\n",
    "\n",
    "\\begin{equation}\\begin{pmatrix} n \\\\ k\\end{pmatrix} = \\frac{n!}{k!(n-k)!}\\end{equation}\n",
    "\n",
    "The exclamation points indicate *factorials* - the product of all positive integers less than or equal to the specified integer  (with 0! having a value of 1).\n",
    "\n",
    "In the case of our <sub>3</sub>C<sub>1</sub> calculation, this means:\n",
    "\n",
    "\\begin{equation}\\begin{pmatrix} 3 \\\\ 1\\end{pmatrix} = \\frac{3!}{1!(3 - 1)!} = \\frac{3!}{1!\\times2!} =\\frac{3 \\times 2 \\times 1}{1 \\times(2 \\times 1)} = \\frac{6}{2} = 3 \\end{equation}\n",
    "\n",
    "That seems like a lot of work to find the number of successful experiments, but now that you know this general formula, you can use it to calculate the number of sample points for any value of *k* from any set of *n* cases. Let's use it to find the possibility of two successful *heads* out of 3 coin flips:\n",
    "\n",
    "\\begin{equation}P(x=2) = \\frac{_{3}C_{2}}{8}\\end{equation}\n",
    "\n",
    "Let's work out the number of combinations for <sub>3</sub>C<sub>2</sub>\n",
    "\n",
    "\\begin{equation}_{3}C_{2} = \\frac{3!}{2!(3 - 2)!} = \\frac{6}{2 \\times 1} = \\frac{6}{2} = 3\\end{equation}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{equation}P(x=2) = \\frac{3}{8}\\end{equation}\n",
    "\n",
    "Finally, what's the probability that all three flips were *heads*?\n",
    "\n",
    "\\begin{equation}P(x=3) = \\frac{_{3}C_{3}}{8}\\end{equation}\n",
    "\n",
    "\\begin{equation}_{3}C_{3} = \\frac{3!}{3!(3 - 3)!} = \\frac{6}{6} = 1\\end{equation}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{equation}P(x=3) = \\frac{1}{8}\\end{equation}\n",
    "\n",
    "In Python, there are a number of modules you can use to find the *n choose k* combinations, including the *scipy.special.**comb*** function. \n",
    "\n",
    "In our coin flipping experiment, there is an equal probability of success and failure; so the probability calculations are relatively simple, and you may notice that there's a symmetry to the probability for each possible value of the binomial variable, as you can see by running the following Python code. You can increase the value of the **trials** variable to verify that no matter how many times we toss the coin, the probabilities of getting *heads* (or *tails* for that matter) form a symmetrical distribution, because there's an equal probability of success and failure in each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy import special as sps\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "trials = 3\n",
    "\n",
    "possibilities = 2**trials\n",
    "x = np.array(range(0, trials+1))\n",
    "\n",
    "p = np.array([sps.comb(trials, i, exact=True)/possibilities for i in x])\n",
    "\n",
    "# Set up the graph\n",
    "plt.xlabel('Successes')\n",
    "plt.ylabel('Probability')\n",
    "plt.bar(x, p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allowing for Bias\n",
    "Previously, we calculated the probability for each possible value of a random variable by simply dividing the number of combinations for that value by the total number of possible outcomes. This works if the probability of the event being tested is equal for failure and success; but of course, not all experiments have an equal chance of success or failure. Some include a bias that makes success more or less likely - so we need to be a little more thorough in our calculations to allow for this.\n",
    "\n",
    "Suppose you're flying off to some exotic destination, and you know that there's a one in four chance that the airport security scanner will trigger a random search for each passenger that goes though. If you watch five passengers go through the scanner, how many will be stopped for a random search?\n",
    "\n",
    "It's tempting to think that there's a one in four chance, so a quarter of the passengers will be stopped; but remember that the searches are triggered randomly for thousands of passengers that pass through the airport each day. It's possible that none of the next five passengers will be searched; all five of them will be searched, or some other value in between will be searched. \n",
    "\n",
    "Even though the probabilities of being searched or not searched are not the same, this is still a binomial variable. There are a fixed number of independent experiments (five passengers passing through the security scanner), the outcome of each experiment is either success (a search is triggered) or failure (no search is triggered), and the probability of being searched does not change for each passenger.\n",
    "\n",
    "There are five experiments in which a passenger goes through the security scanner, let's call this **n**.\n",
    "\n",
    "For each passenger, the probability of being searched is <sup>1</sup>/<sub>4</sub> or 0.25. We'll call this **p**. \n",
    "\n",
    "The complement of **p** (in other words, the probability of *not* being searched) is **1-p**, in this case <sup>3</sup>/<sub>4</sub> or 0.75.\n",
    "\n",
    "So, what's the probability that out of our **n** experiments, three result in a search (let's call that **k**) and the remaining ones (there will be **n**-**k** of them, which is two) don't?\n",
    "\n",
    "- The probability of three passengers being searched is 0.25 x 0.25 x 0.25 which is the same as 0.25<sup>3</sup>. Using our generic variables, this is **p<sup>k</sup>**.\n",
    "- The probability that the rest don't get searched is 0.75 x 0.75, or 0.75<sup>2</sup>. In terms of our variables, this is **1-p<sup>(n-k)</sup>**.\n",
    "- The combined probability of three searchs and two non-searches is therefore 0.25<sup>3</sup> x 0.75<sup>2</sup> (approximately 0.088). Using our variables, this is:\n",
    "\n",
    "\\begin{equation}p^{k}(1-p)^{(n-k)}\\end{equation}\n",
    "\n",
    "This formula enables us to calculate the probability for a single combination of ***n*** passengers in which ***k*** experiments had a successful outcome. In this case, it enables us to calculate that the probability of three passengers out of five being searched is approximately 0.088. However, we need to consider that there are multiple ways this can happen. The first three passengers could get searched; or the last three; or the first, third, and fifth, or any other possible combination of 3 from 5.\n",
    "\n",
    "There are two possible outcomes for each experiment; so the total number of possible combinations of five passengers being searched or not searched is 2<sup>5</sup> or 32. So within those 32 sets of possible result combinations, how many have three searches? We can use the <sub>n</sub>C<sub>k</sub> formula to calculate this:\n",
    "\n",
    "\\begin{equation}_{5}C_{3} = \\frac{5!}{3!(5 - 3)!} = \\frac{120}{6\\times 4} = \\frac{120}{24} = 5\\end{equation}\n",
    "\n",
    "So 5 out of our 32 combinations had 3 searches and 2 non-searches.\n",
    "\n",
    "To find the probability of any combination of 3 searches out of 5 passengers, we need to multiply the number of possible combinations by the probability for a single combination - in this case <sup>5</sup>/<sub>32</sub> x 0.088, which is 0.01375, or 13.75%.\n",
    "\n",
    "So our complete formula to calculate the probabilty of ***k*** events from ***n*** experiments with probability ***p*** is:\n",
    "\n",
    "\\begin{equation}P(x=k)  = \\frac{n!}{k!(n-k)!} p^{k}(1-p)^{(n-k)}\\end{equation}\n",
    "\n",
    "This is known as the *General Binomial Probability Formula*, and we use it to calculate the *probability mass function* (or *PMF*) for a binomial variable. In other words, the we can use it to calculate the probability for each possible value for the variable and use that information to determine the relative frequency of the variable values as a distribution.\n",
    "\n",
    "In Python, the *scipy.stats.**binom.pmf*** function encapsulates the general binomial probability formula, and you can use it to calculate the probability of a random variable having a specific value (***k***) for a given number of experiments (***n***) where the event being tested has a given probability (***p***), as demonstrated in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.stats import binom\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n = 5\n",
    "p = 0.25\n",
    "x = np.array(range(0, n+1))\n",
    "\n",
    "prob = np.array([binom.pmf(k, n, p) for k in x])\n",
    "\n",
    "# Set up the graph\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability')\n",
    "plt.bar(x, prob)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the bar chart that with this small value for ***n***, the distribution is right-skewed.\n",
    "\n",
    "Recall that in our coin flipping experiment, when the probability of failure vs success was equal, the resulting distribution was symmetrical. With an unequal probability of success in each experiment, the bias has the effect of skewing the overall probability mass.\n",
    "\n",
    "However, try increasing the value of ***n*** in the code above to 10, 20, and 50; re-running the cell each time. With more observations, the *central limit theorem* starts to take effect and the distribution starts to look more symmetrical - with enough observations it starts to look like a *normal* distribution.\n",
    "\n",
    "There is an important distinction here - the *normal* distribution applies to *continuous* variables, while the *binomial* distribution applies to *discrete* variables. However, the similarities help in a number of statistical contexts where the number of observations (experiments) is large enough for the *central limit theorem* to make the distribution of binomial variable values behave like a *normal* distribution.\n",
    "\n",
    "### Working with the Binomial Distribution\n",
    "Now that you know how to work out a binomial distribution for a repeated experiment, it's time to take a look at some statistics that will help us quantify some aspects of probability.\n",
    "\n",
    "Let's increase our ***n*** value to 100 so that we're looking at the number of searches per 100 passengers. This gives us the binomial distribution graphed by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.stats import binom\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n = 100\n",
    "p = 0.25\n",
    "x = np.array(range(0, n+1))\n",
    "\n",
    "prob = np.array([binom.pmf(k, n, p) for k in x])\n",
    "\n",
    "# Set up the graph\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability')\n",
    "plt.bar(x, prob)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean (Expected Value)\n",
    "We can calculate the mean of the distribution like this:\n",
    "\n",
    "\\begin{equation}\\mu  = np\\end{equation}\n",
    "\n",
    "So for our airport passengers, this is:\n",
    "\n",
    "\\begin{equation}\\mu  = 100 \\times 0.25 = 25\\end{equation}\n",
    "\n",
    "When we're talking about a probability distribution, the mean is usually referred to as the *expected value*. In this case, for any 100 passengers we can reasonably expect 25 of them to be searched.\n",
    "\n",
    "#### Variance and Standard Deviation\n",
    "Obviously, we can't search a quarter of a passenger - the expected value reflects the fact that there is variation, and indicates an average value for our binomial random variable. To get an indication of how much variability there actually is in this scenario, we can can calculate the variance and standard deviation.\n",
    "\n",
    "For variance of a binomial probability distribution, we can use this formula:\n",
    "\n",
    "\\begin{equation}\\sigma^{2}  = np(1-p)\\end{equation}\n",
    "\n",
    "So for our airport passengers:\n",
    "\n",
    "\\begin{equation}\\sigma^{2}  = 100 \\times 0.25 \\times 0.75 = 18.75\\end{equation}\n",
    "\n",
    "To convert this to standard deviation we just take the square root:\n",
    "\n",
    "\\begin{equation}\\sigma  = \\sqrt{np(1-p)}\\end{equation}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{equation}\\sigma  = \\sqrt{18.75} \\approx 4.33 \\end{equation}\n",
    "\n",
    "So for every 100 passengers, we can expect 25 searches with a standard deviation of 4.33\n",
    "\n",
    "In Python, you can use the ***mean***, ***var***, and ***std*** functions from the *scipy.stats.**binom*** package to return binomial distribution statistics for given values of *n* and *p*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "n = 100\n",
    "p = 0.25\n",
    "\n",
    "print(binom.mean(n,p))\n",
    "print(binom.var(n,p))\n",
    "print(binom.std(n,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Sampling Distributions\n",
    "Most statistical analysis involves working with distributions - usually of sample data.\n",
    "\n",
    "## Sampling and Sampling Distributions\n",
    "As we discussed earlier, when working with statistics, we usually base our calculations on a sample and not the full population of data. This means we need to allow for some variation between the sample statistics and the true parameters of the full population.\n",
    "\n",
    "In the previous example, we knew the probability that a security search would be triggered was 25%, so it's pretty easy to calculate that the expected value for a random variable indicating the number of searches per 100 passengers is 25. What if we hadn't known the probability of a search? How could we estimate the expected mean number of searches for a given number of passengers based purely on sample data collected by observing passengers go through security?\n",
    "\n",
    "### Creating a Proportion Distribution from a Sample\n",
    "We know that the each passenger will either be searched or not searched, and we can assign the values ***0*** (for not searched) and ***1*** (for searched) to these outcomes. We can conduct a Bernoulli trial in which we sample 16 passengers and calculate the fraction (or *proportion*) of passengers that were searched (which we'll call ***p***), and the remaining proportion of passengers (which are the ones who weren't searched, and can be calculated as ***1-p***).\n",
    "\n",
    "Let's say we record the following values for our 16-person sample:\n",
    "\n",
    "    0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0\n",
    "\n",
    "In this sample, there were 3 searches out of 16 passengers; which as a proportion is <sup>3</sup>/<sub>16</sub> or 0.1875. This is our proportion (or **p**); but because we know that this is based on a sample, we call it **p&#770;** (or p-hat). The remaining proportion of passengers is 1-p; in this case 1 - 0.1875, which is 0.8125.\n",
    "\n",
    "The data itself is *qualitative* (categorical) - we're indicating \"no search\" or \"search\"; but because we're using numeric values (0 and 1), we can treat these values as numeric and create a binomial distribution from them - it's the simplest form of a binomial distribution - a Bernoulli distribution with two values.\n",
    "\n",
    "Because we're treating the results as a numberic distribution, we can also calculate statistics like *mean* and *standard deviation*:\n",
    "\n",
    "To calculate these, you can use the following formulae:\n",
    "\n",
    "\\begin{equation}\\mu_{\\hat{p}} = \\hat{p}\\end{equation}\n",
    "\n",
    "\\begin{equation}\\sigma_{\\hat{p}} = \\sqrt{\\hat{p}(1-\\hat{p})}\\end{equation}\n",
    "\n",
    "The mean is just the value of **p&#770;**, so in the case of the passenger search sample it is 0.1875.\n",
    "\n",
    "The standard deviation is calculated as:\n",
    "\n",
    "\\begin{equation}\\sigma_{\\hat{p}} = \\sqrt{0.1875 \\times 0.8125} \\approx 0.39\\end{equation}\n",
    "\n",
    "We can use Python to plot the sample distribution and calculate the mean and standard deviation of our sample like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "searches = np.array([0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0])\n",
    "\n",
    "# Set up the graph\n",
    "plt.xlabel('Search Results')\n",
    "plt.ylabel('Frequency')\n",
    "plt.hist(searches)\n",
    "plt.show()\n",
    "print('Mean: ' + str(np.mean(searches)))\n",
    "print('StDev: ' + str(np.std(searches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When talking about probability, the *mean* is also known as the *expected value*; so based on our single sample of 16 passengers, should we expect the proportion of searched passengers to be 0.1875 (18.75%)?\n",
    "\n",
    "Well, using a single sample like this can be misleading because the number of searches can vary with each sample. Another person observing 100 passengers may get a (very) different result from you. One way to address this problem is to take multiple samples and combine the resulting means to form a *sampling* distribution. This will help us ensure that the distribution and statistics of our sample data is closer to the true values; even if we can't measure the full population.\n",
    "\n",
    "### Creating a Sampling Distribution of a Sample Proportion\n",
    "So, let's collect mulitple 16-passenger samples - here are the resulting sample proportions for 12 samples:\n",
    "\n",
    "| Sample | Result |\n",
    "|--------|--------|\n",
    "| p&#770;<sub>1</sub>| 0.1875 |\n",
    "| p&#770;<sub>2</sub>| 0.2500 |\n",
    "| p&#770;<sub>3</sub>| 0.3125 |\n",
    "| p&#770;<sub>4</sub>| 0.1875 |\n",
    "| p&#770;<sub>5</sub>| 0.1250 |\n",
    "| p&#770;<sub>6</sub>| 0.3750 |\n",
    "| p&#770;<sub>7</sub>| 0.2500 |\n",
    "| p&#770;<sub>8</sub>| 0.1875 |\n",
    "| p&#770;<sub>9</sub>| 0.3125 |\n",
    "| p&#770;<sub>10</sub>| 0.2500 |\n",
    "| p&#770;<sub>11</sub>| 0.2500 |\n",
    "| p&#770;<sub>12</sub>| 0.3125 |\n",
    "\n",
    "We can plot these as a sampling distribution like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "searches = np.array([0.1875,0.25,0.3125,0.1875,0.125,0.375,0.25,0.1875,0.3125,0.25,0.25,0.3125])\n",
    "\n",
    "# Set up the graph\n",
    "plt.xlabel('Search Results')\n",
    "plt.ylabel('Frequency')\n",
    "plt.hist(searches)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Central Limit Theorem\n",
    "You saw previously with the binomial probability distribution, with a large enough sample size (the *n* value indicating the number of binomial experiments), the distribution of values for a random variable started to form an approximately *normal* curve. This is the effect of the *central limit theorem*, and it applies to any distribution of sample data if the size of the sample is large enough. For our airport passenger data, if we collect a large enough number of samples, each based on a large enough number of passenger observations, the sampling distribution will be approximately normal. The larger the sample size, the closer to a perfect *normal* distribution the data will be, and the less variance around the mean there will be.\n",
    "\n",
    "Run the cell below to see a simulated distribution created by 10,000 random 100-passenger samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n, p, s = 100, 0.25, 10000\n",
    "df = pd.DataFrame(np.random.binomial(n,p,s)/n, columns=['p-hat'])\n",
    "\n",
    "# Plot the distribution as a histogram\n",
    "means = df['p-hat']\n",
    "means.plot.hist(title='Simulated Sampling Distribution')  \n",
    "plt.show()\n",
    "print ('Mean: ' + str(means.mean()))\n",
    "print ('Std: ' + str(means.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Standard Error of a Sampling Distribution of Proportion\n",
    "The sampling distribution is created from the means of multiple samples, and its mean is therefore the mean of all the sample means. For a distribution of proportion means, this is considered to be the same as **p** (the population mean). In the case of our passenger search samples, this is 0.25.\n",
    "\n",
    "Because the sampling distribution is based on means, and not totals, its standard deviation is referred to as its *standard error*, and its formula is:\n",
    "\n",
    "\\begin{equation}\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}\\end{equation}\n",
    "\n",
    "In this formula, *n* is the size of each sample; and we divide by this to correct for the error introduced by the average values used in the sampling distribution. In this case, our samples were based on observing 16-passengers, so:\n",
    "\n",
    "\\begin{equation}\\sigma_{\\hat{p}} = \\sqrt{\\frac{0.25 \\times 0.75}{16}} \\approx 0.11\\end{equation}\n",
    "\n",
    "In our simulation of 100-passenger samples, the mean remains 0.25. The standard error is:\n",
    "\n",
    "\\begin{equation}\\sigma_{\\hat{p}} = \\sqrt{\\frac{0.25 \\times 0.75}{100}} \\approx 0.043\\end{equation}\n",
    "\n",
    "Note that the effect of the central limit theorem is that as you increase the number and/or size of samples, the mean remains constant but the amount of variance around it is reduced.\n",
    "\n",
    "Being able to calculate the mean (or *expected value*) and standard error is useful, because we can apply these to what we know about an approximately normal distribution to estimate probabilities for particular values. For example, we know that in a normal distribution, around 95.4% of the values are within two standard deviations of the mean. If we apply that to our sampling distribution of ten thousand 100-passenger samples, we can determine that the proportion of searched passengers in 95.4% of the samples was between 0.164 (16.4%) and 0.336 (36.6%).\n",
    "\n",
    "How do we know this?\n",
    "\n",
    "We know that the mean is ***0.25*** and the standard error (which is the same thing as the standard deviation for our sampling distribution) is ***0.043***. We also know that because this is a *normal* distribution, ***95.4%*** of the data lies within two standard deviations (so 2 x 0.043) of the mean, so the value for 95.4% of our samples is 0.25 &plusmn; (*plus or minus*) 0.086.\n",
    "\n",
    "The *plus or minus* value is known as the *margin of error*, and the range of values within it is known as a *confidence interval* - we'll look at these in more detail later. For now, run the following cell to see a visualization of this interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n, p, s = 100, 0.25, 10000\n",
    "df = pd.DataFrame(np.random.binomial(n,p,s)/n, columns=['p-hat'])\n",
    "\n",
    "# Plot the distribution as a histogram\n",
    "means = df['p-hat']\n",
    "m = means.mean()\n",
    "sd = means.std()\n",
    "moe1 = m - (sd * 2)\n",
    "moe2 = m + (sd * 2)\n",
    "\n",
    "\n",
    "means.plot.hist(title='Simulated Sampling Distribution')  \n",
    "\n",
    "plt.axvline(m, color='red', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(moe1, color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(moe2, color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Sampling Distribution of Sample Means\n",
    "In the previous example, we created a sampling distribution of proportions; which is a suitable way to handle discrete values, like the number of passengers searched or not searched. When you need to work with continuous data, you use slightly different formulae to work with the sampling distribution.\n",
    "\n",
    "For example, suppose we want to examine the weight of the hand luggage carried by each passenger. It's impractical to weigh every bag that is carried through security, but we could weigh one or more samples, for say, 5 passengers at a time, on twelve occassions. We might end up with some data like this:\n",
    "\n",
    "| Sample | Weights |\n",
    "|--------|---------|\n",
    "| 1      | [4.020992,2.143457,2.260409,2.339641,4.699211] |\n",
    "| 2      | [3.38532,4.438345,3.170228,3.499913,4.489557] |\n",
    "| 3      | [3.338228,1.825221,3.53633,3.507952,2.698669] |\n",
    "| 4      | [2.992756,3.292431,3.38148,3.479455,3.051273] |\n",
    "| 5      | [2.969977,3.869029,4.149342,2.785682,3.03557] |\n",
    "| 6      | [3.138055,2.535442,3.530052,3.029846,2.881217] |\n",
    "| 7      | [1.596558,1.486385,3.122378,3.684084,3.501813] |\n",
    "| 8      | [2.997384,3.818661,3.118434,3.455269,3.026508] |\n",
    "| 9      | [4.078268,2.283018,3.606384,4.555053,3.344701] |\n",
    "| 10     | [2.532509,3.064274,3.32908,2.981303,3.915995] |\n",
    "| 11     | [4.078268,2.283018,3.606384,4.555053,3.344701] |\n",
    "| 12     | [2.532509,3.064274,3.32908,2.981303,3.915995] |\n",
    "\n",
    "Just as we did before, we could take the mean of each of these samples and combine them to form a sampling distribution of the sample means (which we'll call **<span style=\"text-decoration: overline;\">X</span>**, and which will contain a mean for each sample, which we'll label x&#772;<sub>n</sub>):\n",
    "\n",
    "| Sample | Mean Weight |\n",
    "|--------|---------|\n",
    "| x&#772;<sub>1</sub> | 3.092742  |\n",
    "| x&#772;<sub>2</sub> | 3.7966726 |\n",
    "| x&#772;<sub>3</sub> | 2.98128   |\n",
    "| x&#772;<sub>4</sub> | 3.239479  |\n",
    "| x&#772;<sub>5</sub> | 3.36192   |\n",
    "| x&#772;<sub>6</sub> | 3.0229224 |\n",
    "| x&#772;<sub>7</sub> | 2.6782436 |\n",
    "| x&#772;<sub>8</sub> | 3.2832512 |\n",
    "| x&#772;<sub>9</sub> | 3.5734848 |\n",
    "| x&#772;<sub>10</sub> | 3.1646322 |\n",
    "| x&#772;<sub>11</sub> | 3.5734848 |\n",
    "| x&#772;<sub>12</sub> | 3.1646322 |\n",
    "\n",
    "We can plot the distribution for the sampling distribution like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "meanweights = np.array([3.092742,\n",
    "                        3.7966726,\n",
    "                        2.98128,\n",
    "                        3.239479,\n",
    "                        3.36192,\n",
    "                        3.0229224,\n",
    "                        2.6782436,\n",
    "                        3.2832512,\n",
    "                        3.5734848,\n",
    "                        3.1646322,\n",
    "                        3.5734848,\n",
    "                        3.1646322])\n",
    "\n",
    "# Set up the graph\n",
    "plt.xlabel('Mean Weights')\n",
    "plt.ylabel('Frequency')\n",
    "plt.hist(meanweights, bins=6)\n",
    "plt.show()\n",
    "\n",
    "print('Mean: ' + str(meanweights.mean()))\n",
    "print('Std: ' + str(meanweights.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, as we increase the sample size, the central limit theorem ensures that our sampling distribution starts to approximate a normal distribution. Our current distribution is based on the means generated from twelve samples, each containing 5 weight observations. Run the following code to see a distribution created from a simulation of 10,000 samples each containing weights for 500 passengers:\n",
    "\n",
    ">This may take a few minutes to run. The code is not the most efficient way to generate a sample distribution, but it reflects the principle that our sampling distribution is made up of the means from multiple samples. In reality, you could simulate the sampling by just creating a single sample from the ***random.normal*** function with a larger ***n*** value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mu, sigma, n = 3.2, 1.2, 500\n",
    "samples = list(range(0, 10000))\n",
    "\n",
    "# data will hold all of the sample data\n",
    "data = np.array([])\n",
    "\n",
    "# sampling will hold the means of the samples\n",
    "sampling = np.array([])\n",
    "\n",
    "# Perform 10,000 samples\n",
    "for s in samples:\n",
    "    # In each sample, get 500 data points from a normal distribution\n",
    "    sample = np.random.normal(mu, sigma, n)\n",
    "    data = np.append(data,sample)\n",
    "    sampling = np.append(sampling,sample.mean())\n",
    "\n",
    "# Create a dataframe with the sampling of means\n",
    "df = pd.DataFrame(sampling, columns=['mean'])\n",
    "\n",
    "# Plot the distribution as a histogram\n",
    "means = df['mean']\n",
    "means.plot.hist(title='Simulated Sampling Distribution', bins=100)  \n",
    "plt.show()\n",
    "\n",
    "# Print the Mean and StdDev for the full sample and for the sampling distribution\n",
    "print('Sample Mean: ' + str(data.mean()))\n",
    "print('Sample StdDev: ' + str(data.std()))\n",
    "print ('Sampling Mean: ' + str(means.mean()))\n",
    "print ('Sampling StdErr: ' + str(means.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Variance of the Sampling Distribution\n",
    "\n",
    "The following variables are printed beneath the histogram:\n",
    "\n",
    "- **Sample Mean**: This is the mean for the complete set of sample data - all 10,000 x 500 bag weights.\n",
    "- **Sample StdDev**: This is the standard deviation for the complete set of sample data - all 10,000 x 500 bag weights.\n",
    "- **Sampling Mean**: This is the mean for the sampling distribution - the means of the means!\n",
    "- **Sampling StdErr**: This is the standard deviation (or *standard error*) for the sampling distribution\n",
    "\n",
    "If we assume that **X** is a random variable representing every possible bag weight, then its mean (indicated as **&mu;<sub>x</sub>**) is the population mean (**&mu;**). The mean of the **<span style=\"text-decoration: overline;\">X</span>** sampling distribution (which is indicated as **&mu;<sub>x&#772;</sub>**) is considered to have the same value. Or, as an equation:\n",
    "\n",
    "\\begin{equation}\\mu_{x} = \\mu_{\\bar{x}}\\end{equation}\n",
    "\n",
    "In this case, the full population mean is unknown (unless we weigh every bag in the world!), but we do have the mean of the full set of sample observations we collected (**x&#772;**), and if we check the values generated by Python for the sample mean and the sampling mean, they're more or less the same: around 3.2.\n",
    "\n",
    "To find the standard deviation of the sample mean, which is technically the *standard error*, we can use this formula:\n",
    "\n",
    "\\begin{equation}\\sigma_{\\bar{x}}  = \\frac{\\sigma}{\\sqrt{n}}\\end{equation}\n",
    "\n",
    "In this formula, ***&sigma;*** is the population standard deviation and ***n*** is the size of each sample.\n",
    "\n",
    "Since our the population standard deviation is unknown, we can use the full sample standard deviation instead:\n",
    "\n",
    "\\begin{equation}SE_{\\bar{x}} \\approx \\frac{s}{\\sqrt{n}}\\end{equation}\n",
    "\n",
    "In this case, the standard deviation of our set of sample data is around 1.2, and we have used 500 variables in each sample to calculate our sample means, so:\n",
    "\n",
    "\\begin{equation}SE_{\\bar{x}} \\approx \\frac{1.2}{\\sqrt{500}} = \\frac{1.2}{22.36} \\approx 0.053\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals\n",
    "A confidence interval is a range of values around a sample statistic within which we are confident that the true parameter lies. For example, our bag weight sampling distribution is based on samples of the weights of bags carried by passengers through our airport security line. We know that the mean weight (the *expected value* for the weight of a bag) in our sampling distribution is 3.2, and we assume this is also the population mean for all bags; but how confident can we be that the true mean weight of all carry-on bags is close to the value?\n",
    "\n",
    "Let's start to put some precision onto these terms. We could state the question another way. What's the range of weights within which are confident that the mean weight of a carry-on bag will be 95% of the time? To calculate this, we need to determine the range of values within which the population mean weight is likely to be in 95% of samples. This is known as a *confidence interval*; and it's based on the Z-scores inherent in a normal distribution.\n",
    "\n",
    "Confidence intervals are expressed as a sample statistic &plusmn; (*plus or minus*) a margin of error. To calculate the margin of error, you need to determine the confidence level you want to find (for example, 95%), and determine the Z score that marks the threshold above or below which the values that are *not* within the chosen interval reside. For example, to calculate a 95% confidence interval, you need the critical Z scores that exclude 5% of the values under the curve; with 2.5% of them being lower than the values in the confidence interval range, and 2.5% being higher. In a normal distribution, 95% of the area under the curve is between a Z score of &plusmn; 1.96. The following table shows the critical Z values for some other popular confidence interval ranges:\n",
    "\n",
    "| Confidence  | Z Score |\n",
    "|-------------|---------|\n",
    "| 90%         | 1.645   |\n",
    "| 95%         | 1.96    |\n",
    "| 99%         | 2.576   |\n",
    "\n",
    "\n",
    "To calculate a confidence interval around a sample statistic, we simply calculate the *standard error* for that statistic as described previously, and multiply this by the approriate Z score for the confidence interval we want.\n",
    "\n",
    "To calculate the 95% confidence interval margin of error for our bag weights, we multiply our standard error of 0.053 by the Z score for a 95% confidence level, which is 1.96:\n",
    "\n",
    "\\begin{equation}MoE = 0.053 \\times 1.96 = 0.10388 \\end{equation}\n",
    "\n",
    "So we can say that we're confident that the population mean weight is in the range of the sample mean &plusmn; 0.10388 with 95% confidence. Thanks to the central limit theorem, if we used an even bigger sample size, the confidence interval would become smaller as the amount of variance in the distribution is reduced. If the number of samples were infinite, the standard error would be 0 and the confidence interval would become a certain value that reflects the true mean weight for all carry-on bags:\n",
    "\n",
    "\\begin{equation}\\lim_{n \\to \\infty} \\frac{\\sigma}{\\sqrt{n}} = 0\\end{equation}\n",
    "\n",
    "\n",
    "In Python, you can use the *scipy.stats.**norm.interval*** function to calculate a confidence interval for a normal distribution. Run the following code to recreate the sampling distribution for bag searches with the same parameters, and display the 95% confidence interval for the mean (again, this may take some time to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "mu, sigma, n = 3.2, 1.2, 500\n",
    "samples = list(range(0, 10000))\n",
    "\n",
    "# data will hold all of the sample data\n",
    "data = np.array([])\n",
    "\n",
    "# sampling will hold the means of the samples\n",
    "sampling = np.array([])\n",
    "\n",
    "# Perform 10,000 samples\n",
    "for s in samples:\n",
    "    # In each sample, get 500 data points from a normal distribution\n",
    "    sample = np.random.normal(mu, sigma, n)\n",
    "    data = np.append(data,sample)\n",
    "    sampling = np.append(sampling,sample.mean())\n",
    "\n",
    "# Create a dataframe with the sampling of means\n",
    "df = pd.DataFrame(sampling, columns=['mean'])\n",
    "\n",
    "# Get the Mean, StdDev, and 95% CI of the means\n",
    "means = df['mean']\n",
    "m = means.mean()\n",
    "sd = means.std()\n",
    "ci = stats.norm.interval(0.95, m, sd)\n",
    "\n",
    "# Plot the distribution, mean, and CI\n",
    "means.plot.hist(title='Simulated Sampling Distribution', bins=100) \n",
    "plt.axvline(m, color='red', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(ci[0], color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(ci[1], color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "# Print the Mean, StdDev and 95% CI\n",
    "print ('Sampling Mean: ' + str(m))\n",
    "print ('Sampling StdErr: ' + str(sd))\n",
    "print ('95% Confidence Interval: ' + str(ci))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Sample, One-Sided Tests\n",
    "Our students have completed their school year, and been asked to rate their statistics class on a scale between -5 (terrible) and 5 (fantastic). The statistics class is taught online to tens of thousands of students, so to assess its success, we'll take a random sample of 50 ratings.\n",
    "\n",
    "Run the following code to draw 50 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(123)\n",
    "lo = np.random.randint(-5, -1, 6)\n",
    "mid = np.random.randint(0, 3, 38)\n",
    "hi = np.random.randint(4, 6, 6)\n",
    "sample = np.append(lo,np.append(mid, hi))\n",
    "print(\"Min:\" + str(sample.min()))\n",
    "print(\"Max:\" + str(sample.max()))\n",
    "print(\"Mean:\" + str(sample.mean()))\n",
    "\n",
    "plt.hist(sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A question we might immediately ask is: \"how do students tend to like the class\"? In this case, possible ratings were between -5 and 5, with a \"neutral\" score of 0. In other words, if our average score is above zero, then students tend to enjoy the course. \n",
    "\n",
    "In the sample above, the mean score is above 0 (in other words, people liked the class in this data). If you had actually run this course and saw this data, it might lead you to believe that the overall mean rating for this class (i.e., not just the sample) is likely to be positive. \n",
    "\n",
    "There is an important point to be made, though: this is just a sample, and you want to make a statement not just about your sample but the whole population from which it came. In other words, you want to know how the class was received overall, but you only have access to a limited set of data. This often the case when analyzing data. \n",
    "\n",
    "So, how can you test your belief that your positive looking *sample* reflects the fact that the course does tend to get good evaluations, that your *population* mean (not just your sample mean) is positive?\n",
    "\n",
    "We start by defining two hypotheses:\n",
    "\n",
    "* The *null* hypothesis (**H<sub>0</sub>**) is that the population mean for all of the ratings is *not* higher than 0, and the fact that our sample mean is higher than this is due to random chance in our sample selection. \n",
    "* The *alternative* hypothesis (**H<sub>1</sub>**) is that the population mean is actually higher than 0, and the fact that our sample mean is higher than this means that our sample correctly detected this trend. \n",
    "\n",
    "You can write these as mutually exclusive expressions like this:\n",
    "\n",
    "\\begin{equation}H_{0}: \\mu \\le 0 \\\\ H_{1}: \\mu > 0 \\end{equation}\n",
    "\n",
    "So how do we test these hypotheses? Because they are mutually exclusive, if we can show the null is probably not true, then we are safe to reject it and conclude that people really do like our online course. But how do we do that?\n",
    "\n",
    "Well, if the *null* hypothesis is true, the sampling distribution for ratings with a sample size of 50 will be a normal distribution with a mean of 0. Run the following code to visualize this, with the mean of 0 shown as a yellow dashed line.\n",
    "\n",
    "*(The code just generates a normal distribution with a mean of 0 and a standard deviation that makes it approximate a sampling distribution of 50 random means between -5 and 5 - don't worry too much about the actual values, it's just to illustrate the key points!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pop = np.random.normal(0, 1.15, 100000)\n",
    "plt.hist(pop, bins=100)\n",
    "plt.axvline(pop.mean(), color='yellow', linestyle='dashed', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This illustrates all the *sample* results you could get if the null hypothesis was true (that is, the rating population mean is actually 0). Note that if the null hypothesis is true, it's still *possible* to get a sample with a mean ranging from just over -5 to just under 5. The question is how *probable* is it to get a sample with a mean as high we did for our 50-rating sample under the null hypothesis? And how improbable would it *need* to be for us to conclude that the null is, in fact, a poor explanation for our data? \n",
    "\n",
    "Well, we measure distance from the mean in standard deviations, so we need to find out how many standard deviations above the null-hypothesized population mean of 0 our sample mean is, and measure the area under the distribution curve from this point on - that will give us the probability of observing a mean that is *at least* as high as our sample mean. We call the number of standard deviations above the mean where our sample mean is found the *test statistic* (or sometimes just *t-statistic*), and we call the area under the curve from this point (representing the probability of observing a sample mean this high or greater) the *p-value*.\n",
    "\n",
    "So the p-value tells us how probable our sample mean is when the null is true, but we need to set a threshold under which we consider this to be too improbable to be explained by random chance alone. We call this threshold our *critical value*, and we usually indicate it using the Greek letter alpha (**&alpha;**). You can use any value you think is appropriate for **&alpha;** - commonly a value of 0.05 (5%) is used, but there's nothing special about this value.\n",
    "\n",
    "We calculate the t-statistic by performing a statistical test. Technically, when the standard deviation of the population is known, we call it a *z-test* (because a *normal* distribution is often called a *z-distribution* and we measure variance from the mean in multiples of standard deviation known as *z-scores*). When the standard deviation of the population is not known, the test is referred to as a *t-test* and based on an adjusted version of a normal distribution called a *student's t distribution*, in which the distribution is \"flattened\" to allow for more sample variation depending on the sample size. Generally, with a sample size of 30 or more, a t-test is approximately equivalent to a z-test.\n",
    "\n",
    "Specifically, in this case we're performing a *single sample* test (we're comparing the mean from a single sample of ratings against the hypothesized population mean), and it's a *one-tailed* test (we're checking to see if the sample mean is *greater than* the null-hypothesized population mean - in other words, in the *right* tail of the distribution).\n",
    "\n",
    "The general formula for one-tailed, single-sample t-test is:\n",
    "\n",
    "\\begin{equation}t = \\frac{\\bar{x} - \\mu}{s \\div \\sqrt{n}} \\end{equation}\n",
    "\n",
    "In this formula, **x&#772;** is the sample mean, **&mu;** is the population mean, **s** is the standard deviation, and **n** is the sample size. You can think of the numerator of this equation (the expression at the top of the fraction) as a *signal*, and the denominator (the expression at the bottom of the fraction) as being *noise*. The signal measures the difference between the statistic and the null-hypothesized value, and the noise represents the random variance in the data in the form of standard deviation (or standard error). The t-statistic is the ratio of signal to noise, and measures the number of standard errors between the null-hypothesized value and the observed sample mean. A large value tells you that your \"result\" or \"signal\" was much larger than you would typically expect by chance.\n",
    "\n",
    "Fortunately, most programming languages used for statistical analysis include functions to perform a t-test, so you rarely need to manually calculate the results using the formula.\n",
    "\n",
    "Run the code below to run a single-sample t-test comparing our sample mean for ratings to a hypothesized population mean of 0, and visualize the resulting t-statistic on the normal distribution for the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# T-Test\n",
    "t,p = stats.ttest_1samp(sample, 0)\n",
    "# ttest_1samp is 2-tailed, so half the resulting p-value to get a 1-tailed p-value\n",
    "p1 = '%f' % (p/2)\n",
    "print (\"t-statistic:\" + str(t))\n",
    "print(\"p-value:\" + str(p1))\n",
    "\n",
    "# calculate a 90% confidence interval. 10% of the probability is outside this, 5% in each tail\n",
    "ci = stats.norm.interval(0.90, 0, 1.15)\n",
    "plt.hist(pop, bins=100)\n",
    "# show the hypothesized population mean\n",
    "plt.axvline(pop.mean(), color='yellow', linestyle='dashed', linewidth=2)\n",
    "# show the right-tail confidence interval threshold - 5% of propbability is under the curve to the right of this.\n",
    "plt.axvline(ci[1], color='red', linestyle='dashed', linewidth=2)\n",
    "# show the t-statistic - the p-value is the area under the curve to the right of this\n",
    "plt.axvline(pop.mean() + t*pop.std(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot produced by the code above, the yellow line shows the population mean for the null hypothesis. The area under the curve to the right of the red line represents the critical value of 0.05 (or 5%). The magenta line indicates how much higher the sample mean is compared to the hypothesized population mean. This is calculated as the t-statistic (which is printed above the plot) multiplied by the standard deviation. The area under the curve to the right of this encapsulates the p-value calculated by the test (which is also printed above the plot).\n",
    "\n",
    "So what should we conclude from these results?\n",
    "\n",
    "Well, if the p-value is smaller than our critical value of 0.05, that means that under the null hypothesis, the probability of observing a sample mean as high as we did by random chance is low. That's a good sign for us, because it means that our sample is unlikely under the null, and therefore the null is a poor explanation for the data. We can safely *reject* the null hypothesis in favor of the alternative hypothesis - there's enough evidence to suggest that the population mean for our class ratings is greater than 0.\n",
    "\n",
    "Conversely, if the p-value is greater than the critical value, we *fail to reject the null hypothesis* and conclude that the mean rating is not greater than 0. Note that we never actually *accept* the null hypothesis, we just conclude that there isn't enough evidence to reject it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Tailed Tests\n",
    "The previous test was an example of a one-tailed test in which the p-value represents the area under one tail of the distribution curve. In this case, the area in question is under the right tail because the alternative hypothesis we were trying to show was that the true population mean is *greater than* the mean of the null hypothesis scenario.\n",
    "\n",
    "Suppose we restated our hypotheses like this:\n",
    "* The *null* hypothesis (**H<sub>0</sub>**) is that the population mean for all of the ratings is 0, and the fact that our sample mean is higher or lower than this can be explained by random chance in our sample selection.\n",
    "* The *alternative* hypothesis (**H<sub>1</sub>**) is that the population mean is not equal to 0.\n",
    "\n",
    "We can write these as mutually exclusive expressions like this:\n",
    "\n",
    "\\begin{equation}H_{0}: \\mu = 0 \\\\ H_{1}: \\mu \\neq 0 \\end{equation}\n",
    "\n",
    "Why would we do this? Well, in the test we performed earlier, we could only reject the null hypothesis if we had really *positive* ratings, but what if our sample data looked really *negative*? It would be a mistake to turn around and run a one-tailed test the other way, for negative ratings. Instead, we conduct a test designed for such a question: a two-tailed test.\n",
    "\n",
    "In a two-tailed test, we are willing to reject the null hypothesis if the result is significantly *greater* or *lower* than the null hypothesis. Our critical value (5%) is therefore split in two: the top 2.5% of the curve and the bottom 2.5% of the curve. As long as our test statistic is in that region, we are in the extreme 5% of values (p < .05) and we reject the null hypothesis. In other words, our p-value now needs to be below .025, but it can be in either tail of the distribution. For convenience, we usually \"double\" the p-value in a two-tailed test so that we don't have to remember this rule and still compare against .05 (this is known as a \"two-tailed p-value\"). In fact, it is assumed this has been done in all statistical analyses unless stated otherwise. \n",
    "\n",
    "The following code shows the results of a two-tailed, single sample test of our class ratings. Note that the ***ttest_1samp*** function in the ***stats*** library returns a 2-tailed p-value by default (which is why we halved it in the previous example).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# T-Test\n",
    "t,p = stats.ttest_1samp(sample, 0)\n",
    "print (\"t-statistic:\" + str(t))\n",
    "# ttest_1samp is 2-tailed\n",
    "print(\"p-value:\" + '%f' % p)\n",
    "# calculate a 95% confidence interval. 50% of the probability is outside this, 2.5% in each tail\n",
    "ci = stats.norm.interval(0.95, 0, 1.15)\n",
    "plt.hist(pop, bins=100)\n",
    "# show the hypothesized population mean\n",
    "plt.axvline(pop.mean(), color='yellow', linestyle='dashed', linewidth=2)\n",
    "# show the confidence interval thresholds - 5% of propbability is under the curve outside these.\n",
    "plt.axvline(ci[0], color='red', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(ci[1], color='red', linestyle='dashed', linewidth=2)\n",
    "# show the t-statistic thresholds - the p-value is the area under the curve outside these\n",
    "plt.axvline(pop.mean() - t*pop.std(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(pop.mean() + t*pop.std(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that our 2-tailed p-value was clearly less than 0.05; so We reject the null hypothesis.\n",
    "\n",
    "You may note that doubling the p-value in a two-tailed test makes it harder to reject the null. This is true; we require more evidence because we are asking a more complicated question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Two-Sample Tests\n",
    "In both of the previous examples, we compared a statistic from a single data sample to a null-hypothesized population parameter. Sometimes you might want to compare two samples against one another.\n",
    "\n",
    "For example, let's suppose that some of the students who took the statistics course had previously studied mathematics, while other students had no previous math experience. You might hypothesize that the grades of students who had previously studied math are significantly higher than the grades of students who had not.\n",
    "\n",
    "* The *null* hypothesis (**H<sub>0</sub>**) is that the population mean grade for students with previous math studies is not greater than the population mean grade for students without any math experience, and the fact that our sample mean for math students is higher than our sample mean for non-math students can be explained by random chance in our sample selection.\n",
    "* The *alternative* hypothesis (**H<sub>1</sub>**) is that the population mean grade for students with previous math studies is greater than the population mean grade for students without any math experience.\n",
    "\n",
    "We can write these as mutually exclusive expressions like this:\n",
    "\n",
    "\\begin{equation}H_{0}: \\mu_{1} \\le \\mu_{2} \\\\ H_{1}: \\mu_{1} > \\mu_{2} \\end{equation}\n",
    "\n",
    "This is a one-sided test that compares two samples. To perform this test, we'll take two samples. One sample contains 100 grades for students who have previously studied math, and the other sample contains 100 grades for students with no math experience.\n",
    "\n",
    "We won't go into the test-statistic formula here, but it essentially the same as the one above, adapted to include information from both samples. We can easily test this in most software packages using the command for an \"independent samples\" t-test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(123)\n",
    "nonMath = np.random.normal(66.0, 1.5, 100)\n",
    "math = np.random.normal(66.55, 1.5, 100)\n",
    "print(\"non-math sample mean:\" + str(nonMath.mean()))\n",
    "print(\"math sample mean:\" + str(math.mean()))\n",
    "\n",
    "# Independent T-Test\n",
    "t,p = stats.ttest_ind(math, nonMath)\n",
    "# ttest_ind is 2-tailed, so half the resulting p-value to get a 1-tailed p-value\n",
    "p1 = '%f' % (p/2)\n",
    "print(\"t-statistic:\" + str(t))\n",
    "print(\"p-value:\" + str(p1))\n",
    "\n",
    "pop = np.random.normal(nonMath.mean(), nonMath.std(), 100000)\n",
    "# calculate a 90% confidence interval. 10% of the probability is outside this, 5% in each tail\n",
    "ci = stats.norm.interval(0.90, nonMath.mean(), nonMath.std())\n",
    "plt.hist(pop, bins=100)\n",
    "# show the hypothesized population mean\n",
    "plt.axvline(pop.mean(), color='yellow', linestyle='dashed', linewidth=2)\n",
    "# show the right-tail confidence interval threshold - 5% of propbability is under the curve to the right of this.\n",
    "plt.axvline(ci[1], color='red', linestyle='dashed', linewidth=2)\n",
    "# show the t-statistic - the p-value is the area under the curve to the right of this\n",
    "plt.axvline(pop.mean() + t*pop.std(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can interpret the results of this test the same way as for the previous single-sample, one-tailed test. If the p-value (the area under the curve to the right of the magenta line) is smaller than our critical value (**&alpha;**) of 0.05 (the area under the curve to the right of the red line), then the difference can't be explained by chance alone; so we can reject the null hypothesis and conclude that students with previous math experience perform better on average than students without.\n",
    "\n",
    "Alternatively, you could always compare two groups and *not* specify a direction (i.e., two-tailed). If you did this, as above, you could simply double the p-value (now .001), and you would see you could still reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired Tests\n",
    "In the two-sample test we conduced previously, the samples were independent; in other words there was no relatioship between the observations in the first sample and the observations in the second sample. Sometimes you might want to compare statistical differences between related observations before and after some change that you believe might influence the data.\n",
    "\n",
    "For example, suppose our students took a mid-term exam, and later took and end-of-term exam. You might hypothesise that the students will improve their grades in the end-of-term exam, after they've undertaken additional study. We could test for a general improvement on average across all students with a two-sample independent test, but a more appropriate test would be to compare the two test scores for each individual student.\n",
    "\n",
    "To accomplish this, we need to create two samples; one for scores in the mid-term, exam, the other for scores in the end-of-term exam. Then we need to compare the samples in such a way that each pair of observations for the same student are compared to one another. \n",
    "\n",
    "This is known as a paired-samples t-test or a dependent-samples t-test. Technically, it tests whether the *changes* tend to be in the positive or negative direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(123)\n",
    "midTerm = np.random.normal(59.45, 1.5, 100)\n",
    "endTerm = np.random.normal(60.05, 1.5, 100)\n",
    "\n",
    "# Paired (related) test\n",
    "t,p = stats.ttest_rel(endTerm, midTerm)\n",
    "# ttest_rel is 2-tailed, so half the resulting p-value to get a 1-tailed p-value\n",
    "p1 = '%f' % (p/2)\n",
    "print(\"t-statistic:\" + str(t))\n",
    "print(\"p-value:\" + str(p1))\n",
    "\n",
    "pop = np.random.normal(midTerm.mean(), midTerm.std(), 100000)\n",
    "# calculate a 90% confidence interval. 10% of the probability is outside this, 5% in each tail\n",
    "ci = stats.norm.interval(0.90, midTerm.mean(), midTerm.std())\n",
    "plt.hist(pop, bins=100)\n",
    "# show the hypothesized population mean\n",
    "plt.axvline(pop.mean(), color='yellow', linestyle='dashed', linewidth=2)\n",
    "# show the right-tail confidence interval threshold - 5% of propbability is under the curve to the right of this.\n",
    "plt.axvline(ci[1], color='red', linestyle='dashed', linewidth=2)\n",
    "# show the t-statistic - the p-value is the area under the curve to the right of this\n",
    "plt.axvline(pop.mean() + t*pop.std(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our sample, we see that scores did in fact improve, so we can we reject the null hypothesis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
